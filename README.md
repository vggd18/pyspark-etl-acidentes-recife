# Data Lakehouse com PySpark, Delta Lake e Dados de Acidentes de TrÃ¢nsito do Recife

### Resumo

Este projeto demonstra a construÃ§Ã£o de um pipeline de ETL (Extract, Transform, Load) de ponta-a-ponta utilizando PySpark e Delta Lake. O objetivo Ã© ingerir mÃºltiplos arquivos CSV anuais de acidentes de trÃ¢nsito da cidade do Recife, lidar com inconsistÃªncias de schema (*schema drift*), aplicar um processo robusto de limpeza e enriquecimento e, finalmente, criar mÃºltiplos **Data Marts** com modelo dimensional (Star Schema) prontos para consumo por ferramentas de Business Intelligence (BI).

A arquitetura implementada segue o padrÃ£o **Medallion (Bronze, Silver, Gold)**, uma abordagem moderna para a organizaÃ§Ã£o de um Data Lakehouse que garante qualidade, governanÃ§a e performance.

-----

### Objetivo do Projeto

O principal objetivo Ã© simular um cenÃ¡rio real de engenharia de dados, demonstrando competÃªncias em:

* IngestÃ£o e processamento de dados em lote (*batch*) a partir de mÃºltiplas fontes com schemas inconsistentes.
* ImplementaÃ§Ã£o de um pipeline de dados resiliente e documentado.
* Limpeza, validaÃ§Ã£o de regras de negÃ³cio e enriquecimento de dados (*feature engineering*).
* OrganizaÃ§Ã£o de um Data Lakehouse com a arquitetadura MedalhÃ£o.
* Modelagem de dados para BI, incluindo a criaÃ§Ã£o de um **Star Schema** e Data Marts com granularidades diferentes.
* DemonstraÃ§Ã£o de recursos avanÃ§ados de governanÃ§a de dados do Delta Lake (ACID, Time Travel, Auditoria).

-----

### Arquitetura Utilizada: Medallion

O pipeline Ã© estruturado em trÃªs camadas lÃ³gicas, cada uma representada por tabelas Delta particionadas:

* **Camada Bronze (Bruta):**
    * **FunÃ§Ã£o:** IngestÃ£o dos dados crus de mÃºltiplos arquivos CSV, servindo como uma cÃ³pia fiel e histÃ³rica da fonte. Um schema unificado Ã© aplicado para consolidar os diferentes layouts dos arquivos.
    * **Tabela:** `bronze/acidentes`

* **Camada Silver (Limpa e Enriquecida):**
    * **FunÃ§Ã£o:** ContÃ©m os dados da camada Bronze apÃ³s passarem por um processo rigoroso de limpeza (padronizaÃ§Ã£o de texto, tratamento de nulos, correÃ§Ã£o de tipos), validaÃ§Ã£o (remoÃ§Ã£o de duplicatas, filtros de regras de negÃ³cio) e enriquecimento (criaÃ§Ã£o de colunas de data e hora). Ã‰ a nossa "fonte Ãºnica da verdade".
    * **Tabela:** `silver/acidentes_limpos`

* **Camada Gold (Agregada e Modelada):**
    * **FunÃ§Ã£o:** Apresenta os dados da camada Silver em modelos otimizados para consultas analÃ­ticas. ContÃ©m mÃºltiplos Data Marts para diferentes tipos de anÃ¡lise.
    * **Tabelas:**
        * **Data Mart 1 (Star Schema):** `fact_acidentes`, `dim_tempo`, `dim_localizacao`, `dim_tipo_acidente`.
        * **Data Mart 2 (VeÃ­culos):** `fact_acidente_veiculo`, `dim_vehicle`.

-----

### ğŸ”¨ Ferramentas e Tecnologias

* **Linguagem:** Python
* **Processamento de Dados:** Apache Spark 3.5.1
* **Camada de Armazenamento:** Delta Lake 3.2.0
* **Ambiente de Desenvolvimento:** Google Colab / Jupyter Notebook
* **Bibliotecas Python:** `pyspark`, `delta-spark`

-----

### ğŸ“‚ Estrutura dos Artefatos

```
.
â”œâ”€â”€ delta_lake/                   # Raiz do nosso Data Lakehouse, gerado pela execuÃ§Ã£o
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ acidentes/
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ acidentes_limpos/
â”‚   â””â”€â”€ gold/
â”‚       â”œâ”€â”€ dim_localizacao/
â”‚       â”œâ”€â”€ dim_tempo/
â”‚       â”œâ”€â”€ dim_tipo_acidente/
â”‚       â”œâ”€â”€ dim_vehicle/
â”‚       â”œâ”€â”€ fact_acidentes/
â”‚       â””â”€â”€ fact_acidente_veiculo/
â”œâ”€â”€ data/                         # Onde os arquivos CSV de origem sÃ£o armazenados
â”‚   â”œâ”€â”€ acidentes_2019.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ etl_acidentes_recife.ipynb    # Notebook principal com todo o cÃ³digo do pipeline
â””â”€â”€ README.md                     # Este arquivo
```

-----

### ğŸš€ Como Executar o Projeto

1.  **Abra o Notebook:** Abra o arquivo `etl_acidentes_recife.ipynb` no Google Colab.
2.  **Execute a CÃ©lula de Download:** Execute a primeira cÃ©lula de cÃ³digo na seÃ§Ã£o `1.1` para baixar os arquivos de dados para a pasta `data/`.
3.  **Execute Todas as CÃ©lulas:** Execute as cÃ©lulas restantes na ordem apresentada. As bibliotecas e configuraÃ§Ãµes necessÃ¡rias sÃ£o instaladas no inÃ­cio do notebook.

-----

### ğŸ“ˆ Etapas do Pipeline Implementadas

1.  **ConfiguraÃ§Ã£o do Ambiente:** InstalaÃ§Ã£o das dependÃªncias e inicializaÃ§Ã£o da `SparkSession` com o catÃ¡logo Delta Lake habilitado.

2.  **Camada Bronze - IngestÃ£o:**
    * AnÃ¡lise de *schema drift* entre os arquivos anuais.
    * DefiniÃ§Ã£o de um `StructType` unificado.
    * ImplementaÃ§Ã£o de um loop de ingestÃ£o robusto com o padrÃ£o **Read, Rename, UnionByName**.
    * Enriquecimento com metadados (`source_file`, `year`).
    * Escrita da tabela `bronze.acidentes` no formato Delta, particionada por ano.

3.  **Camada Silver - Limpeza, ValidaÃ§Ã£o e Enriquecimento:**
    * Leitura da tabela Bronze.
    * **Limpeza:** PadronizaÃ§Ã£o de *case* (`upper`), tratamento de valores nulos (`na.fill`).
    * **CorreÃ§Ã£o de Tipos:** ConversÃ£o de colunas numÃ©ricas lidas como texto (devido a formataÃ§Ã£o com vÃ­rgula) para `IntegerType`, usando `regexp_replace` e `cast`.
    * **Enriquecimento:** CriaÃ§Ã£o de atributos de data/tempo (`month`, `day_of_week`, `periodo_do_dia`).
    * **ValidaÃ§Ã£o:** RemoÃ§Ã£o de duplicatas (`dropDuplicates`) e aplicaÃ§Ã£o de filtros para garantir regras de negÃ³cio.
    * Escrita da tabela `silver.acidentes_limpos`.

4.  **Camada Gold - Modelagem Dimensional:**
    * Leitura da tabela Silver.
    * **Data Mart 1 (Star Schema):**
        * CriaÃ§Ã£o de Tabelas de DimensÃ£o (`Dim_Tempo`, `Dim_Localizacao`, `Dim_TipoAcidente`) com chaves substitutas.
        * CriaÃ§Ã£o da Tabela Fato (`Fato_Acidentes`) atravÃ©s de `JOIN`s, com mÃ©tricas de negÃ³cio.
    * **Data Mart 2 (AnÃ¡lise de VeÃ­culos):**
        * CriaÃ§Ã£o da `Dim_Vehicle`.
        * AplicaÃ§Ã£o de uma transformaÃ§Ã£o **Unpivot** (`stack`) para mudar a granularidade dos dados.
        * CriaÃ§Ã£o de uma segunda Tabela Fato (`Fato_Acidentes_Veiculos`) de granularidade fina.
    * Salvamento de todas as tabelas Fato e DimensÃ£o na camada Gold.

5.  **DemonstraÃ§Ã£o dos Recursos do Delta Lake:**
    * **ACID Transactions:** ExecuÃ§Ã£o de uma operaÃ§Ã£o de `UPDATE` na tabela Silver para padronizar dados de forma segura.
    * **Auditoria e HistÃ³rico:** Uso do comando `DESCRIBE HISTORY` para auditar as transaÃ§Ãµes na tabela.
    * **Time Travel:** DemonstraÃ§Ã£o da capacidade de consultar uma versÃ£o anterior da tabela, antes da correÃ§Ã£o ser aplicada.

-----

### ğŸ’¡ PrÃ³ximos Passos

Este projeto pode ser estendido com as seguintes funcionalidades:

* **OrquestraÃ§Ã£o:** Automatizar a execuÃ§Ã£o do pipeline usando uma ferramenta como Airflow, Prefect ou Mage.
* **VisualizaÃ§Ã£o:** Conectar a camada Gold a uma ferramenta de BI (como Power BI, Tableau ou Looker Studio) para criar um dashboard interativo.
* **Testes de Qualidade:** Integrar uma biblioteca como Great Expectations para criar validaÃ§Ãµes de dados mais robustas.
* **Infraestrutura como CÃ³digo:** Usar Terraform para provisionar a infraestrutura necessÃ¡ria para rodar este pipeline na nuvem (ex: Databricks, AWS Glue, Google Dataproc).

-----

### ğŸ‘¨â€ğŸ’» Autor

**Vitor Gabriel Gomes Dias**

* [LinkedIn](https://www.linkedin.com/in/vggd/)
* [GitHub](https://github.com/vggd18)
