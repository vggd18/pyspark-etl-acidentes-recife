# Data Lakehouse com PySpark, Delta Lake e Dados de Acidentes de TrÃ¢nsito do Recife

### Resumo

Este projeto demonstra a construÃ§Ã£o de um pipeline de ETL (Extract, Transform, Load) de ponta-a-ponta utilizando PySpark e Delta Lake. O objetivo Ã© ingerir mÃºltiplos arquivos CSV anuais de acidentes de trÃ¢nsito da cidade do Recife, lidar com inconsistÃªncias de schema (*schema drift*), aplicar um processo robusto de limpeza e enriquecimento e, finalmente, criar um **Data Mart** com modelo dimensional (Star Schema) pronto para consumo por ferramentas de Business Intelligence (BI).

A arquitetura implementada segue o padrÃ£o **Medallion (Bronze, Silver, Gold)**, uma abordagem moderna para a organizaÃ§Ã£o de um Data Lakehouse que garante qualidade, governanÃ§a e performance.

-----

### Objetivo do Projeto

O principal objetivo Ã© simular um cenÃ¡rio real de engenharia de dados, demonstrando competÃªncias em:

* IngestÃ£o e processamento de dados em lote (*batch*) a partir de mÃºltiplas fontes com schemas inconsistentes.
* ImplementaÃ§Ã£o de um pipeline de dados resiliente e documentado.
* Limpeza, validaÃ§Ã£o de regras de negÃ³cio e enriquecimento de dados (*feature engineering*).
* OrganizaÃ§Ã£o de um Data Lakehouse com a arquitetura MedalhÃ£o.
* Modelagem de dados para BI (Star Schema com tabelas Fato e DimensÃ£o).
* UtilizaÃ§Ã£o dos recursos do Delta Lake (ACID, versionamento, particionamento).

-----

### Arquitetura Utilizada: Medallion

O pipeline Ã© estruturado em trÃªs camadas lÃ³gicas, cada uma representada por tabelas Delta particionadas:

* **Camada Bronze (Bruta):**
    * **FunÃ§Ã£o:** IngestÃ£o dos dados crus de mÃºltiplos arquivos CSV, servindo como uma cÃ³pia fiel e histÃ³rica da fonte. Um schema unificado Ã© aplicado para consolidar os diferentes layouts dos arquivos.
    * **Tabela:** `bronze.acidentes`

* **Camada Silver (Limpa e Enriquecida):**
    * **FunÃ§Ã£o:** ContÃ©m os dados da camada Bronze apÃ³s passarem por um processo rigoroso de limpeza (padronizaÃ§Ã£o de texto, tratamento de nulos, correÃ§Ã£o de tipos), validaÃ§Ã£o (remoÃ§Ã£o de duplicatas, filtros de regras de negÃ³cio) e enriquecimento (criaÃ§Ã£o de colunas de data e hora). Ã‰ a nossa "fonte Ãºnica da verdade".
    * **Tabela:** `silver.acidentes_limpos`

* **Camada Gold (Agregada e Modelada):**
    * **FunÃ§Ã£o:** Apresenta os dados da camada Silver em um modelo dimensional (Star Schema) otimizado para consultas analÃ­ticas. ContÃ©m tabelas Fato e DimensÃ£o que formam um Data Mart para anÃ¡lise de acidentes.
    * **Tabelas:** `fato_acidentes`, `dim_tempo`, `dim_localizacao`, `dim_tipo_acidente`.

-----

### ğŸ”¨ Ferramentas e Tecnologias

* **Linguagem:** Python
* **Processamento de Dados:** Apache Spark 3.5.1
* **Camada de Armazenamento:** Delta Lake 3.2.0
* **Ambiente de Desenvolvimento:** Google Colab / Jupyter Notebook
* **Bibliotecas Python:** `pyspark`, `delta-spark`

-----

### ğŸ“‚ Estrutura dos Artefatos

```
.
â”œâ”€â”€ delta_lake/                   # Raiz do nosso Data Lakehouse, gerado pela execuÃ§Ã£o
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ acidentes/
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ acidentes_limpos/
â”‚   â””â”€â”€ gold/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ data/                         # Onde os arquivos CSV de origem sÃ£o armazenados
â”‚   â”œâ”€â”€ acidentes_2019.csv
â”‚   â”œâ”€â”€ acidentes_2020.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ etl_acidentes_recife.ipynb    # Notebook principal com todo o cÃ³digo do pipeline
â””â”€â”€ README.md                     # Este arquivo
```

-----

### ğŸš€ Como Executar o Projeto

1.  **Abra o Notebook:** Abra o arquivo `etl_acidentes_recife.ipynb` no Google Colab.
2.  **Execute a CÃ©lula de Download:** Execute a primeira cÃ©lula de cÃ³digo na seÃ§Ã£o `1.1` para baixar os arquivos de dados para a pasta `data/`.
3.  **Execute Todas as CÃ©lulas:** Execute as cÃ©lulas restantes na ordem apresentada. As bibliotecas `pyspark` e `delta-spark` sÃ£o instaladas e configuradas no inÃ­cio do notebook.

-----

### ğŸ“ˆ Etapas do Pipeline Implementadas

O notebook estÃ¡ dividido nas seguintes etapas:

1.  **ConfiguraÃ§Ã£o do Ambiente:** InstalaÃ§Ã£o das dependÃªncias e inicializaÃ§Ã£o da `SparkSession` com o catÃ¡logo Delta Lake habilitado.

2.  **Camada Bronze - IngestÃ£o:**
    * AnÃ¡lise de *schema drift* entre os diferentes arquivos anuais.
    * DefiniÃ§Ã£o de um `StructType` unificado para criar um superconjunto de todas as colunas.
    * ImplementaÃ§Ã£o de um loop de ingestÃ£o robusto com o padrÃ£o **Read, Rename, UnionByName** para consolidar todos os arquivos.
    * Enriquecimento com metadados (`source_file`, `year`).
    * Escrita da tabela `bronze.acidentes` no formato Delta, particionada por ano.

3.  **Camada Silver - Limpeza, ValidaÃ§Ã£o e Enriquecimento:**
    * Leitura da tabela Bronze.
    * **Limpeza:** PadronizaÃ§Ã£o de *case* em colunas de texto (`upper`), tratamento de valores nulos em campos categÃ³ricos (`na.fill`).
    * **CorreÃ§Ã£o de Tipos:** ConversÃ£o de colunas numÃ©ricas que foram lidas como texto (devido a formataÃ§Ã£o com vÃ­rgula) para `IntegerType`, usando `regexp_replace` e `cast`.
    * **Enriquecimento:** CriaÃ§Ã£o de atributos de data e tempo (`month`, `day_of_week`, `periodo_do_dia`) para facilitar anÃ¡lises.
    * **ValidaÃ§Ã£o:** RemoÃ§Ã£o de registros 100% duplicados (`dropDuplicates`) e aplicaÃ§Ã£o de filtros para garantir regras de negÃ³cio (ex: contagens nÃ£o negativas).
    * Escrita da tabela `silver.acidentes_limpos`.

4.  **Camada Gold - Modelagem Dimensional:**
    * Leitura da tabela Silver.
    * CriaÃ§Ã£o de **Tabelas de DimensÃ£o** (`Dim_Tempo`, `Dim_Localizacao`, `Dim_TipoAcidente`) com chaves substitutas (*surrogate keys*).
    * CriaÃ§Ã£o da **Tabela Fato** (`Fato_Acidentes`) atravÃ©s de `JOIN`s, contendo chaves estrangeiras e mÃ©tricas de negÃ³cio.

-----

### ğŸ’¡ PrÃ³ximos Passos

Este projeto pode ser estendido com as seguintes funcionalidades:

* **OrquestraÃ§Ã£o:** Automatizar a execuÃ§Ã£o do pipeline usando uma ferramenta como Airflow, Prefect ou Mage.
* **VisualizaÃ§Ã£o:** Conectar a camada Gold a uma ferramenta de BI (como Power BI, Tableau ou Looker Studio) para criar um dashboard interativo.
* **Testes de Qualidade:** Integrar uma biblioteca como Great Expectations para criar validaÃ§Ãµes de dados mais robustas e declarativas.
* **Infraestrutura como CÃ³digo:** Usar Terraform ou outra ferramenta para provisionar a infraestrutura necessÃ¡ria para rodar este pipeline na nuvem.

-----

### ğŸ‘¨â€ğŸ’» Autor

**[Vitor Dias]**

* [LinkedIn](https://www.linkedin.com/in/vggd)
* [GitHub](https://github.com/vggd18)
```
