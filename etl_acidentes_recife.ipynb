{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyOVyRPS1qIicC/CRlPDe/PR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vggd18/pyspark-etl-acidentes-recife/blob/main/etl_acidentes_recife.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto de Data Lakehouse: Acidentes de Trânsito do Recife\n",
        "\n",
        "**Objetivo:** Construir um pipeline de dados completo (ETL) utilizando PySpark e Delta Lake para processar dados abertos de acidentes de trânsito da cidade do Recife. O projeto segue a arquitetura Medalhão (Bronze, Silver, Gold) para criar um Data Lakehouse robusto, otimizado e pronto para análises.\n",
        "\n",
        "**Ferramentas:** PySpark, Delta Lake, Python\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CKsrA3LRd2Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuração do Ambiente (Environment Setup)\n",
        "\n",
        "Nesta seção, preparamos nosso ambiente de desenvolvimento no Google Colab, instalando as bibliotecas necessárias e configurando a sessão Spark com suporte ao Delta Lake."
      ],
      "metadata": {
        "id": "YcM2dhRsd438"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Download dos Dados de Origem (Source Data)\n",
        "\n",
        "A primeira etapa do pipeline é a ingestão dos dados brutos. Aqui, fazemos o download dos arquivos CSV anuais (2019-2024) diretamente do [Portal de Dados Abertos da Prefeitura do Recife](http://dados.recife.pe.gov.br/dataset/acidentes-de-transito-com-e-sem-vitimas)."
      ],
      "metadata": {
        "id": "8f4QrjlUhaUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jr0OGJZGc4z5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20e631d-1e50-4517-f22f-1d461a7eec5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive acidentes_2019.csv downloaded successfully.\n",
            "Archive acidentes_2020.csv downloaded successfully.\n",
            "Archive acidentes_2021.csv downloaded successfully.\n",
            "Archive acidentes_2022.csv downloaded successfully.\n",
            "Archive acidentes_2023.csv downloaded successfully.\n",
            "Archive acidentes_2024.csv downloaded successfully.\n",
            "All files downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "urls = {\n",
        "  \"2019\": \"http://dados.recife.pe.gov.br/dataset/44087d2d-73b5-4ab3-9bd8-78da7436eed1/resource/3531bafe-d47d-415e-b154-a881081ac76c/download/acidentes-2019.csv\",\n",
        "  \"2020\": \"http://dados.recife.pe.gov.br/dataset/44087d2d-73b5-4ab3-9bd8-78da7436eed1/resource/fc1c8460-0406-4fff-b51a-e79205d1f1ab/download/acidentes_2020-novo.csv\",\n",
        "  \"2021\": \"http://dados.recife.pe.gov.br/dataset/44087d2d-73b5-4ab3-9bd8-78da7436eed1/resource/2caa8f41-ccd9-4ea5-906d-f66017d6e107/download/acidentes2021.csv\",\n",
        "  \"2022\": \"http://dados.recife.pe.gov.br/dataset/44087d2d-73b5-4ab3-9bd8-78da7436eed1/resource/971e0228-fa9c-4a42-b360-c842b29d2f56/download/acidentes2022.csv\",\n",
        "  \"2023\": \"http://dados.recife.pe.gov.br/dataset/44087d2d-73b5-4ab3-9bd8-78da7436eed1/resource/d26b864b-0f7b-403e-b142-fd9989acaaf5/download/acidentes2023.csv\",\n",
        "  \"2024\": \"http://dados.recife.pe.gov.br/dataset/44087d2d-73b5-4ab3-9bd8-78da7436eed1/resource/29afbf42-a36c-475c-8b75-761e17e67679/download/acidentes2024.csv\"\n",
        "}\n",
        "\n",
        "output_dir = 'data'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for year, url in urls.items():\n",
        "  file_name = f\"acidentes_{year}.csv\"\n",
        "  file_path =  os.path.join(output_dir, file_name)\n",
        "\n",
        "  response = requests.get(url)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    with open(file_path, 'wb') as file:\n",
        "      file.write(response.content)\n",
        "    print(f\"Archive {file_name} downloaded successfully.\")\n",
        "  else:\n",
        "    print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
        "\n",
        "print(\"All files downloaded successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Inicialização da Sessão Spark (Spark Session)\n",
        "\n",
        "Configuramos uma sessão Spark habilitada para o Delta Lake. Utilizamos a função `configure_spark_with_delta_pip` que garante a correta configuração das dependências Java (JARs), resolvendo os desafios de compatibilidade do ambiente."
      ],
      "metadata": {
        "id": "99x-01frg66S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.1 delta-spark==3.2.0 -q"
      ],
      "metadata": {
        "id": "He-KhOAogXfG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import *\n",
        "\n",
        "builder = (\n",
        "  SparkSession.builder.appName(\"EtlAcidentesRecife\")\n",
        "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        ")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "print(\"SparkSession and Delta Lake configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrV8TGwvhCBJ",
        "outputId": "08616d10-01c6-4f89-fd90-c3458f1b3936"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession and Delta Lake configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Camada Bronze: Ingestão e Armazenamento dos Dados Brutos\n",
        "\n",
        "O objetivo da Camada Bronze é criar uma cópia fiel, histórica e imutável dos dados de origem. Nesta etapa, lemos todos os arquivos CSV, lidamos com as inconsistências de schema e salvamos os dados em uma única tabela Delta particionada."
      ],
      "metadata": {
        "id": "6r1Ge_RYjEuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Análise de Consistência do Schema (Schema Drift Analysis)\n",
        "\n",
        "Antes de carregar todos os arquivos, é uma boa prática verificar se eles possuem a mesma estrutura. Nosso script de análise revelou um **Schema Drift** significativo: os nomes e o número de colunas mudam ao longo dos anos. Esta descoberta é crucial e justifica a necessidade de um schema unificado manual."
      ],
      "metadata": {
        "id": "oFRfPIM-jIok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_acidentes = spark.read \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"delimiter\", \";\") \\\n",
        "  .csv('data/acidentes_2019.csv')"
      ],
      "metadata": {
        "id": "FoNzGbl1jKL3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_acidentes.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpX2ifrwj54a",
        "outputId": "6fb2dab3-d1fc-4378-eade-9c0c5f81bdb3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DATA: date (nullable = true)\n",
            " |-- hora: string (nullable = true)\n",
            " |-- natureza_acidente: string (nullable = true)\n",
            " |-- situacao: string (nullable = true)\n",
            " |-- bairro: string (nullable = true)\n",
            " |-- endereco: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- detalhe_endereco_acidente: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- endereco_cruzamento: string (nullable = true)\n",
            " |-- numero_cruzamento: string (nullable = true)\n",
            " |-- referencia_cruzamento: string (nullable = true)\n",
            " |-- bairro_cruzamento: string (nullable = true)\n",
            " |-- num_semaforo: integer (nullable = true)\n",
            " |-- sentido_via: string (nullable = true)\n",
            " |-- tipo: string (nullable = true)\n",
            " |-- descricao: string (nullable = true)\n",
            " |-- auto: integer (nullable = true)\n",
            " |-- moto: integer (nullable = true)\n",
            " |-- ciclom: integer (nullable = true)\n",
            " |-- ciclista: integer (nullable = true)\n",
            " |-- pedestre: integer (nullable = true)\n",
            " |-- onibus: integer (nullable = true)\n",
            " |-- caminhao: integer (nullable = true)\n",
            " |-- viatura: integer (nullable = true)\n",
            " |-- outros: integer (nullable = true)\n",
            " |-- vitimas: integer (nullable = true)\n",
            " |-- vitimasfatais: integer (nullable = true)\n",
            " |-- acidente_verificado: string (nullable = true)\n",
            " |-- tempo_clima: string (nullable = true)\n",
            " |-- situacao_semaforo: string (nullable = true)\n",
            " |-- sinalizacao: string (nullable = true)\n",
            " |-- condicao_via: string (nullable = true)\n",
            " |-- conservacao_via: string (nullable = true)\n",
            " |-- ponto_controle: string (nullable = true)\n",
            " |-- situacao_placa: string (nullable = true)\n",
            " |-- velocidade_max_via: string (nullable = true)\n",
            " |-- mao_direcao: string (nullable = true)\n",
            " |-- divisao_via1: string (nullable = true)\n",
            " |-- divisao_via2: string (nullable = true)\n",
            " |-- divisao_via3: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_acidentes.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVXYPaJckD_6",
        "outputId": "82d7a6a4-9eab-4a80-fb07-1d3c277cf2f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----------------+----------+-----------+-------------------------------+------+---------------------------+-----------------------------------------------------------+-------------------------------+-----------------+-----------------------------------------------------------+-----------------+------------+-----------+------------------------+-----------------------------------------------------------------------------------+----+----+------+--------+--------+------+--------+-------+------+-------+-------------+-------------------+-----------+-----------------+---------------+------------+---------------+-----------------+--------------+------------------+-----------+----------------+------------+------------+\n",
            "|DATA      |hora    |natureza_acidente|situacao  |bairro     |endereco                       |numero|detalhe_endereco_acidente  |complemento                                                |endereco_cruzamento            |numero_cruzamento|referencia_cruzamento                                      |bairro_cruzamento|num_semaforo|sentido_via|tipo                    |descricao                                                                          |auto|moto|ciclom|ciclista|pedestre|onibus|caminhao|viatura|outros|vitimas|vitimasfatais|acidente_verificado|tempo_clima|situacao_semaforo|sinalizacao    |condicao_via|conservacao_via|ponto_controle   |situacao_placa|velocidade_max_via|mao_direcao|divisao_via1    |divisao_via2|divisao_via3|\n",
            "+----------+--------+-----------------+----------+-----------+-------------------------------+------+---------------------------+-----------------------------------------------------------+-------------------------------+-----------------+-----------------------------------------------------------+-----------------+------------+-----------+------------------------+-----------------------------------------------------------------------------------+----+----+------+--------+--------+------+--------+-------+------+-------+-------------+-------------------+-----------+-----------------+---------------+------------+---------------+-----------------+--------------+------------------+-----------+----------------+------------+------------+\n",
            "|2019-01-01|00:41:00|SEM VÍTIMA       |FINALIZADA|IPSEP      |AV RECIFE                      |NULL  |NULL                       |LADO OPOSTO AO Nº 3257                                     |AV RECIFE                      |NULL             |LADO OPOSTO AO Nº 3257                                     |IPSEP            |NULL        |SUBURBIO   |COLISÃO TRASEIRA        |ART.181, OBSTRUÇÃO DE ENTRADA E SAÍDA DE GARAGEM DO CONDOMINIO                     |2   |NULL|NULL  |NULL    |NULL    |NULL  |NULL    |NULL   |NULL  |0      |NULL         |Longo da via       |Bom        |Não existe       |Perfeito estado|Seca        |Perfeito estado|Não existe       |Não há placas |60 km/h           |Única      |Faixa seccionada|NULL        |NULL        |\n",
            "|2019-01-01|01:37:00|SEM VÍTIMA       |FINALIZADA|BOA VIAGEM |RUA PADRE BERNADINO PESSOA     |NULL  |RUA MINISTRO NELSON HUNGRIA|NULL                                                       |RUA PADRE BERNADINO PESSOA     |NULL             |NULL                                                       |BOA VIAGEM       |NULL        |NULL       |ABALROAMENTO TRANSVERSAL|COLISÃO ENTRE DOIS AUTOS S/V                                                       |2   |NULL|NULL  |NULL    |NULL    |NULL  |NULL    |NULL   |NULL  |0      |NULL         |Cruzamento         |Bom        |Não existe       |Perfeito estado|Seca        |Perfeito estado|Faixa de pedestre|Não há placas |NULL              |Única      |Não existe      |NULL        |NULL        |\n",
            "|2019-01-01|14:20:00|SEM VÍTIMA       |CANCELADA |BOA VIAGEM |AV ENGENHEIRO DOMINGOS FERREIRA|NULL  |RUA DOM JOSE LOPES         |EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQUERDO DA PISTA|AV ENGENHEIRO DOMINGOS FERREIRA|NULL             |EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQUERDO DA PISTA|BOA VIAGEM       |NULL        |NULL       |COLISÃO                 |COLISÃO ENTRE DOIS AUTOS S/V                                                       |2   |NULL|NULL  |NULL    |NULL    |NULL  |NULL    |NULL   |NULL  |0      |NULL         |NULL               |NULL       |NULL             |NULL           |NULL        |NULL           |NULL             |NULL          |NULL              |NULL       |NULL            |NULL        |NULL        |\n",
            "|2019-01-01|02:53:00|SEM VÍTIMA       |CANCELADA |IMBIRIBEIRA|AV GENERAL MAC ARTHUR          |100   |RUA JACY                   |EM FRENTE A ART LED ILUMINAÇÃO                             |AV GENERAL MAC ARTHUR          |100              |EM FRENTE A ART LED ILUMINAÇÃO                             |IMBIRIBEIRA      |NULL        |NULL       |COLISÃO                 |COLISÃO ENTRE DOIS AUTOS S/V                                                       |2   |NULL|NULL  |NULL    |NULL    |NULL  |NULL    |NULL   |NULL  |0      |NULL         |NULL               |NULL       |NULL             |NULL           |NULL        |NULL           |NULL             |NULL          |NULL              |NULL       |NULL            |NULL        |NULL        |\n",
            "|2019-01-01|08:17:00|COM VÍTIMA       |FINALIZADA|JAQUEIRA   |RUA TITO ROSAS                 |63    |NULL                       |ED. JARDINS DA JAQUEIRA                                    |RUA TITO ROSAS                 |63               |ED. JARDINS DA JAQUEIRA                                    |JAQUEIRA         |NULL        |SUBURBIO   |COLISÃO COM CICLISTA    |COLISÃO ENVOLVENDO ÔNIBUS E BICICLETA COM VÍTIMA. EQUIPE DEPAROU-SE COM O ACIDENTE.|NULL|NULL|NULL  |1       |NULL    |1     |NULL    |NULL   |NULL  |1      |NULL         |Longo da via       |Bom        |Não existe       |Perfeito estado|Seca        |Perfeito estado|Não existe       |Não há placas |40 km/h           |Única      |Faixa seccionada|NULL        |NULL        |\n",
            "+----------+--------+-----------------+----------+-----------+-------------------------------+------+---------------------------+-----------------------------------------------------------+-------------------------------+-----------------+-----------------------------------------------------------+-----------------+------------+-----------+------------------------+-----------------------------------------------------------------------------------+----+----+------+--------+--------+------+--------+-------+------+-------+-------------+-------------------+-----------+-----------------+---------------+------------+---------------+-----------------+--------------+------------------+-----------+----------------+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_acidentes.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b2K_6GVBQSo",
        "outputId": "4596ebb8-f983-4658-e7cb-711aa980cc60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+-----------------+---------+---------+-------------------+------------------+-------------------------+--------------------+-------------------+------------------+---------------------+-----------------+------------------+-----------------+--------------------+------------------+------------------+------------------+------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------+-------------------+-----------+-----------------+---------------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+\n",
            "|summary|    hora|natureza_acidente| situacao|   bairro|           endereco|            numero|detalhe_endereco_acidente|         complemento|endereco_cruzamento| numero_cruzamento|referencia_cruzamento|bairro_cruzamento|      num_semaforo|      sentido_via|                tipo|         descricao|              auto|              moto|ciclom|           ciclista|           pedestre|             onibus|           caminhao|           viatura|             outros|            vitimas|vitimasfatais|acidente_verificado|tempo_clima|situacao_semaforo|    sinalizacao|condicao_via|conservacao_via|ponto_controle|situacao_placa|velocidade_max_via|mao_direcao|divisao_via1|divisao_via2|divisao_via3|\n",
            "+-------+--------+-----------------+---------+---------+-------------------+------------------+-------------------------+--------------------+-------------------+------------------+---------------------+-----------------+------------------+-----------------+--------------------+------------------+------------------+------------------+------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------+-------------------+-----------+-----------------+---------------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+\n",
            "|  count|   12046|            12058|    12058|    11914|              12009|              6013|                     4308|                9609|              12002|              6011|                 9606|            11909|              3029|             8293|               12062|             11762|             11140|              2904|    48|                224|                255|               1811|               1224|               122|                210|              12035|           25|               9191|       9706|             9581|           9524|        9673|           9525|          8670|          8733|              2858|       9522|        9130|         908|         150|\n",
            "|   mean|    NULL|             NULL|     NULL|     NULL|               NULL| 1184.249320652174|                     NULL|             2334.75|               NULL|1183.8995922528034|              2334.75|     9.98434034E8|289.15450643776825|           135.25|                NULL|              NULL|1.5841113105924596|1.0561294765840221|   1.0| 1.0089285714285714| 1.0509803921568628| 1.0209828823854223| 1.0212418300653594|1.0245901639344261|  1.061904761904762|0.21545492314083922|          1.0|               NULL|       NULL|             NULL|           NULL|        NULL|           NULL|          NULL|          NULL|              NULL|       NULL|        NULL|        NULL|        NULL|\n",
            "| stddev|    NULL|             NULL|     NULL|     NULL|               NULL|1527.5017701756722|                     NULL|   2635.866761301362|               NULL|1527.1183559269984|    2635.866761301362|             NULL| 217.6663272461742|76.97781065562552|                NULL|              NULL|0.6177047673742271|0.2433057550274722|   0.0|0.09427901670929757|0.23758342837999133|0.14336649028121842|0.15517171249419665| 0.155511140899212|0.24155831906253245|  0.490081563481564|          0.0|               NULL|       NULL|             NULL|           NULL|        NULL|           NULL|          NULL|          NULL|              NULL|       NULL|        NULL|        NULL|        NULL|\n",
            "|    min|00:00:00|            APOIO|CANCELADA|  AFLITOS|1 TRV JUSTICA E PAZ|            0940 A|      1 TRV JUSTICA E PAZ|10 METROS ANTES C...|1 TRV JUSTICA E PAZ|            0940 A| 10 METROS ANTES C...|        998434034|                 1|              191|ABALROAMENTO LONG...|   ,MOTO X CLASSIC|                 0|                 0|     1|                  1|                  1|                  1|                  0|                 1|                  1|                  0|            1|         Cruzamento|        Bom|      Com defeito|       Ilegível|     Molhada| Mal conservada|        Agente|         A-33a|           10 km/h|      Dupla|      Blocos|      Blocos|      Blocos|\n",
            "|    max|48:00:00|     VÍTIMA FATAL| PENDENTE|ÁGUA FRIA|           av norte|                 \\|     VIADUTO TANCREDO ...|ÁREA INTERNA DO S...|           av norte|                 \\| ÁREA INTERNA DO S...|        ÁGUA FRIA|              4038|        ÁGUA FRIA|          TOMBAMENTO|ÕNIBUS X AUTO. S/V|                 6|                 4|     1|                  2|                  3|                  2|                  2|                 2|                  2|                  7|            1|            Viaduto|    Nublado|      Sem defeito|Perfeito estado|        Seca|Perfeito estado|        Outros|          R-6c|           60 km/h|      Única|      Outros|      Outros|      Outros|\n",
            "+-------+--------+-----------------+---------+---------+-------------------+------------------+-------------------------+--------------------+-------------------+------------------+---------------------+-----------------+------------------+-----------------+--------------------+------------------+------------------+------------------+------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------+-------------------+-----------+-----------------+---------------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "data_dir = 'data/'\n",
        "\n",
        "csv_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.csv')])\n",
        "\n",
        "if not csv_files:\n",
        "  print(\"No CSV files found in the directory 'data/'\")\n",
        "else:\n",
        "  base_path = os.path.join(data_dir, csv_files[0])\n",
        "  base_header = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(base_path).columns\n",
        "\n",
        "  equals = True\n",
        "\n",
        "  for f in csv_files[1:]:\n",
        "    current_path = os.path.join(data_dir, f)\n",
        "    current_header = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(current_path).columns\n",
        "    if current_header != base_header:\n",
        "      equals = False\n",
        "      print(f\"\\n!!! ALERT: The header of '{f}' is DIFFERENT! Analysis:\")\n",
        "      base_set = set(base_header)\n",
        "      current_set = set(current_header)\n",
        "      removed_columns = base_set - current_set\n",
        "      if removed_columns:\n",
        "          print(f\"  - Missing columns in this file: {list(removed_columns)}\")\n",
        "      added_columns = current_set - base_set\n",
        "      if added_columns:\n",
        "          print(f\"  - Extra columns found in this file: {list(added_columns)}\")\n",
        "      if len(base_header) != len(current_header):\n",
        "            print(f\"  - Column count diverges: {len(base_header)} in reference vs. {len(current_header)} in this file.\") # Translated\n",
        "      print(\"-\" * 30)\n",
        "\n",
        "  if equals:\n",
        "      print(\"\\nGreat news! All CSV files have the same header.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxP8ymJ9uSDe",
        "outputId": "e4f785cd-4fae-4e75-94be-c8cff54c2e18"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "!!! ALERT: The header of 'acidentes_2020.csv' is DIFFERENT! Analysis:\n",
            "  - Missing columns in this file: ['referencia_cruzamento', 'numero_cruzamento', 'endereco_cruzamento', 'DATA']\n",
            "  - Extra columns found in this file: ['data']\n",
            "  - Column count diverges: 41 in reference vs. 38 in this file.\n",
            "------------------------------\n",
            "\n",
            "!!! ALERT: The header of 'acidentes_2021.csv' is DIFFERENT! Analysis:\n",
            "  - Missing columns in this file: ['endereco_cruzamento', 'referencia_cruzamento', 'DATA', 'numero_cruzamento', 'descricao']\n",
            "  - Extra columns found in this file: ['data']\n",
            "  - Column count diverges: 41 in reference vs. 37 in this file.\n",
            "------------------------------\n",
            "\n",
            "!!! ALERT: The header of 'acidentes_2022.csv' is DIFFERENT! Analysis:\n",
            "  - Missing columns in this file: ['natureza_acidente', 'endereco_cruzamento', 'referencia_cruzamento', 'DATA', 'numero_cruzamento', 'descricao']\n",
            "  - Extra columns found in this file: ['Protocolo', 'data', 'natureza']\n",
            "  - Column count diverges: 41 in reference vs. 38 in this file.\n",
            "------------------------------\n",
            "\n",
            "!!! ALERT: The header of 'acidentes_2023.csv' is DIFFERENT! Analysis:\n",
            "  - Missing columns in this file: ['natureza_acidente', 'endereco_cruzamento', 'referencia_cruzamento', 'DATA', 'numero_cruzamento', 'descricao']\n",
            "  - Extra columns found in this file: ['Protocolo', 'data', 'natureza']\n",
            "  - Column count diverges: 41 in reference vs. 38 in this file.\n",
            "------------------------------\n",
            "\n",
            "!!! ALERT: The header of 'acidentes_2024.csv' is DIFFERENT! Analysis:\n",
            "  - Missing columns in this file: ['natureza_acidente', 'endereco_cruzamento', 'referencia_cruzamento', 'DATA', 'numero_cruzamento', 'descricao']\n",
            "  - Extra columns found in this file: ['Protocolo', 'data', 'natureza']\n",
            "  - Column count diverges: 41 in reference vs. 38 in this file.\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_acidentes2024 = spark.read \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .option(\"delimiter\", \";\") \\\n",
        "  .csv('data/acidentes_2024.csv')\n",
        "\n",
        "df_acidentes2024.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaPigv8DF3QQ",
        "outputId": "8742585e-43e7-4304-d679-b4aaa63052f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+------------+----------+-------+--------------------+-----------------+-------------------------+--------------------+-----------------+------------+-----------+--------------------+----+----+------+--------+--------+------+--------+-------+------+-------+-------------+-------------------+-----------+-----------------+-----------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+\n",
            "|summary|  Protocolo|    natureza|  situacao| bairro|            endereco|           numero|detalhe_endereco_acidente|         complemento|bairro_cruzamento|num_semaforo|sentido_via|                tipo|auto|moto|ciclom|ciclista|pedestre|onibus|caminhao|viatura|outros|vitimas|vitimasfatais|acidente_verificado|tempo_clima|situacao_semaforo|sinalizacao|condicao_via|conservacao_via|ponto_controle|situacao_placa|velocidade_max_via|mao_direcao|divisao_via1|divisao_via2|divisao_via3|\n",
            "+-------+-----------+------------+----------+-------+--------------------+-----------------+-------------------------+--------------------+-----------------+------------+-----------+--------------------+----+----+------+--------+--------+------+--------+-------+------+-------+-------------+-------------------+-----------+-----------------+-----------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+\n",
            "|  count|       5315|        5315|      5315|   5308|                5297|             2419|                      698|                5036|             5308|         199|        377|                5285|5315|5315|  5315|    5315|    5315|  5315|    5315|   5315|  5315|   5315|         5315|                  0|          0|                0|          0|           0|              0|             0|             0|                 0|          0|           0|           0|           0|\n",
            "|   mean|       NULL|        NULL|      NULL|   NULL|                NULL|1425.840475180314|                     NULL|                NULL|             NULL|        NULL|     1987.0|                NULL|NULL|NULL|  NULL|    NULL|    NULL|  NULL|    NULL|   NULL|  NULL|   NULL|         NULL|               NULL|       NULL|             NULL|       NULL|        NULL|           NULL|          NULL|          NULL|              NULL|       NULL|        NULL|        NULL|        NULL|\n",
            "| stddev|       NULL|        NULL|      NULL|   NULL|                NULL|2193.806380777639|                     NULL|                NULL|             NULL|        NULL|       NULL|                NULL|NULL|NULL|  NULL|    NULL|    NULL|  NULL|    NULL|   NULL|  NULL|   NULL|         NULL|               NULL|       NULL|             NULL|       NULL|        NULL|           NULL|          NULL|          NULL|              NULL|       NULL|        NULL|        NULL|        NULL|\n",
            "|    min|202406373,0|  COM VÍTIMA| CANCELADA|AFLITOS|1 TRV DOUTOR SABI...|                1|     1 TRV DOUTOR SABI...|( JORDÃO BAIXO ),...|          AFLITOS|       100,0|       1987|ATROPELAMENTO DE ...| 0,0| 0,0|   0,0|     0,0|     0,0|   0,0|     0,0|    0,0|   0,0|    0,0|          0,0|               NULL|       NULL|             NULL|       NULL|        NULL|           NULL|          NULL|          NULL|              NULL|       NULL|        NULL|        NULL|        NULL|\n",
            "|    max|   299315,0|VÍTIMA FATAL|FINALIZADA|  ZUMBI|VIADUTO ULYSSES G...|               SN|     VDO PAPA JOAO PAU...|sobre a ponte Jos...|            ZUMBI|        96,0|   SUBURBIO|          TOMBAMENTO| 4,0| 3,0|   1,0|     2,0|     2,0|   2,0|     2,0|    2,0|  24,0|    4,0|          6,0|               NULL|       NULL|             NULL|       NULL|        NULL|           NULL|          NULL|          NULL|              NULL|       NULL|        NULL|        NULL|        NULL|\n",
            "+-------+-----------+------------+----------+-------+--------------------+-----------------+-------------------------+--------------------+-----------------+------------+-----------+--------------------+----+----+------+--------+--------+------+--------+-------+------+-------+-------------+-------------------+-----------+-----------------+-----------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Definição do Schema Unificado (Unified Schema Definition)\n",
        "\n",
        "Com base na análise, definimos um **schema unificado e explícito** usando `StructType`. Este schema representa o \"superconjunto\" de todas as colunas encontradas em todos os arquivos, padronizando os nomes (ex: `DATA` para `data`) e definindo os tipos de dados corretos. Manter todas as colunas como `nullable=True` nesta etapa garante que a ingestão não falhe por dados faltantes na origem."
      ],
      "metadata": {
        "id": "9BBIyqqktrns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType\n",
        "\n",
        "unified_schema = StructType([\n",
        "  StructField('data', DateType(), True),\n",
        "  StructField('hora', StringType(), True),\n",
        "  StructField('natureza_acidente', StringType(), True),\n",
        "  StructField('situacao', StringType(), True),\n",
        "  StructField('protocolo', StringType(), True),\n",
        "\n",
        "  StructField('bairro', StringType(), True),\n",
        "  StructField('endereco', StringType(), True),\n",
        "  StructField('numero', StringType(), True),\n",
        "\n",
        "  StructField('detalhe_endereco_acidente', StringType(), True),\n",
        "  StructField('complemento', StringType(), True),\n",
        "  StructField('endereco_cruzamento', StringType(), True),\n",
        "  StructField('numero_cruzamento', StringType(), True),\n",
        "  StructField('referencia_cruzamento', StringType(), True),\n",
        "  StructField('bairro_cruzamento', StringType(), True),\n",
        "  StructField('num_semaforo', IntegerType(), True),\n",
        "  StructField('sentido_via', StringType(), True),\n",
        "  StructField('tipo', StringType(), True),\n",
        "  StructField('descricao', StringType(), True),\n",
        "\n",
        "  StructField('auto', IntegerType(), True),\n",
        "  StructField('moto', IntegerType(), True),\n",
        "  StructField('ciclom', IntegerType(), True),\n",
        "  StructField('ciclista', IntegerType(), True),\n",
        "  StructField('pedestre', IntegerType(), True),\n",
        "  StructField('onibus', IntegerType(), True),\n",
        "  StructField('caminhao', IntegerType(), True),\n",
        "  StructField('viatura', IntegerType(), True),\n",
        "  StructField('outros', IntegerType(), True),\n",
        "  StructField('vitimas', IntegerType(), True),\n",
        "  StructField('vitimas_fatais', IntegerType(), True),\n",
        "\n",
        "  StructField('acidente_verificado', StringType(), True),\n",
        "  StructField('tempo_clima', StringType(), True),\n",
        "  StructField('situacao_semaforo', StringType(), True),\n",
        "  StructField('sinalizacao', StringType(), True),\n",
        "  StructField('condicao_via', StringType(), True),\n",
        "  StructField('conservacao_via', StringType(), True),\n",
        "  StructField('ponto_controle', StringType(), True),\n",
        "  StructField('situacao_placa', StringType(), True),\n",
        "  StructField('velocidade_max_via', IntegerType(), True),\n",
        "  StructField('mao_direcao', StringType(), True),\n",
        "\n",
        "  StructField('divisao_via1', StringType(), True),\n",
        "  StructField('divisao_via2', StringType(), True),\n",
        "  StructField('divisao_via3', StringType(), True)\n",
        "\n",
        "])\n",
        "\n",
        "schema_columns = unified_schema.names\n",
        "print(f\"Your Schema have {len(schema_columns)} columns.\")\n",
        "print(unified_schema)"
      ],
      "metadata": {
        "id": "pukidjastueN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5702995-40cb-40c4-aa78-94a6ef84157b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Schema have 42 columns.\n",
            "StructType([StructField('data', DateType(), True), StructField('hora', StringType(), True), StructField('natureza_acidente', StringType(), True), StructField('situacao', StringType(), True), StructField('protocolo', StringType(), True), StructField('bairro', StringType(), True), StructField('endereco', StringType(), True), StructField('numero', StringType(), True), StructField('detalhe_endereco_acidente', StringType(), True), StructField('complemento', StringType(), True), StructField('endereco_cruzamento', StringType(), True), StructField('numero_cruzamento', StringType(), True), StructField('referencia_cruzamento', StringType(), True), StructField('bairro_cruzamento', StringType(), True), StructField('num_semaforo', IntegerType(), True), StructField('sentido_via', StringType(), True), StructField('tipo', StringType(), True), StructField('descricao', StringType(), True), StructField('auto', IntegerType(), True), StructField('moto', IntegerType(), True), StructField('ciclom', IntegerType(), True), StructField('ciclista', IntegerType(), True), StructField('pedestre', IntegerType(), True), StructField('onibus', IntegerType(), True), StructField('caminhao', IntegerType(), True), StructField('viatura', IntegerType(), True), StructField('outros', IntegerType(), True), StructField('vitimas', IntegerType(), True), StructField('vitimas_fatais', IntegerType(), True), StructField('acidente_verificado', StringType(), True), StructField('tempo_clima', StringType(), True), StructField('situacao_semaforo', StringType(), True), StructField('sinalizacao', StringType(), True), StructField('condicao_via', StringType(), True), StructField('conservacao_via', StringType(), True), StructField('ponto_controle', StringType(), True), StructField('situacao_placa', StringType(), True), StructField('velocidade_max_via', IntegerType(), True), StructField('mao_direcao', StringType(), True), StructField('divisao_via1', StringType(), True), StructField('divisao_via2', StringType(), True), StructField('divisao_via3', StringType(), True)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = 'data'\n",
        "all_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')])\n",
        "\n",
        "all_headers = set()\n",
        "\n",
        "for file_path in all_files:\n",
        "  header = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").csv(file_path).columns\n",
        "  all_headers.update(header)\n",
        "\n",
        "unique_column_list = sorted(list(all_headers))\n",
        "\n",
        "print(f\"Found {len(unique_column_list)} unique columns across all files:\")\n",
        "print(unique_column_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCAqyZft6bW8",
        "outputId": "9d07e9d9-39a5-4433-d4f0-f4c9a97f1b1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 44 unique columns across all files:\n",
            "['DATA', 'Protocolo', 'acidente_verificado', 'auto', 'bairro', 'bairro_cruzamento', 'caminhao', 'ciclista', 'ciclom', 'complemento', 'condicao_via', 'conservacao_via', 'data', 'descricao', 'detalhe_endereco_acidente', 'divisao_via1', 'divisao_via2', 'divisao_via3', 'endereco', 'endereco_cruzamento', 'hora', 'mao_direcao', 'moto', 'natureza', 'natureza_acidente', 'num_semaforo', 'numero', 'numero_cruzamento', 'onibus', 'outros', 'pedestre', 'ponto_controle', 'referencia_cruzamento', 'sentido_via', 'sinalizacao', 'situacao', 'situacao_placa', 'situacao_semaforo', 'tempo_clima', 'tipo', 'velocidade_max_via', 'viatura', 'vitimas', 'vitimasfatais']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_columns_standardized = [header.lower() for header in unique_column_list]\n",
        "print(f\"The source files have {len(source_columns_standardized)} unique standardized columns.\")\n",
        "\n",
        "extra_columns = set(schema_columns) - set(source_columns_standardized)\n",
        "if extra_columns:\n",
        "  print(f\"The extra columns are: {extra_columns}\")\n",
        "else:\n",
        "  print(\"No extra columns found.\")\n",
        "\n",
        "over_columns = set(source_columns_standardized) - set(schema_columns)\n",
        "if over_columns:\n",
        "  print(f\"The missing columns are: {over_columns}\")\n",
        "else:\n",
        "  print(\"No missing columns found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95F7s0XIHjqY",
        "outputId": "e5048cc1-0e4b-42da-a80e-e0faa05e38c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The source files have 44 unique standardized columns.\n",
            "The extra columns are: {'vitimas_fatais'}\n",
            "The missing columns are: {'vitimasfatais', 'natureza'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Carga, Mapeamento e Enriquecimento (Ingestion, Mapping & Enrichment)\n",
        "\n",
        "Implementamos um loop para processar cada arquivo CSV. Para lidar com o Schema Drift, utilizamos o padrão **\"Read, Rename, and UnionByName\"**:\n",
        "1.  **Read:** Lemos cada arquivo individualmente, deixando o Spark inferir os nomes originais do cabeçalho.\n",
        "2.  **Rename:** Renomeamos as colunas inconsistentes para se alinharem ao nosso schema padrão.\n",
        "3.  **UnionByName:** Unimos o DataFrame tratado ao DataFrame final, alinhando as colunas pelo nome.\n",
        "\n",
        "Além disso, enriquecemos os dados com duas colunas de metadados essenciais:\n",
        "- `source_file`: Para rastreabilidade da origem de cada registro.\n",
        "- `year`: Para permitir o particionamento da tabela."
      ],
      "metadata": {
        "id": "RCx_RwdeLJlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bronze_final = spark.createDataFrame([], unified_schema)"
      ],
      "metadata": {
        "id": "rR8_6wRqnbUF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data'\n",
        "all_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')])\n",
        "\n",
        "for file_path in all_files:\n",
        "    df_raw = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option('inferSchema', True).csv(file_path)\n",
        "\n",
        "    df_renamed = df_raw.withColumnRenamed('DATA','data') \\\n",
        "                       .withColumnRenamed('natureza', 'natureza_acidente') \\\n",
        "                       .withColumnRenamed('vitimasfatais', 'vitimas_fatais') \\\n",
        "                       .withColumnRenamed('Protocolo', 'protocolo')\n",
        "\n",
        "    df_bronze_final = df_bronze_final.unionByName(df_renamed, allowMissingColumns=True)"
      ],
      "metadata": {
        "id": "7pe0_W81L98C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_columns = df_bronze_final.columns\n",
        "\n",
        "expected_schema_columns = unified_schema.names\n",
        "\n",
        "missing_columns = set(expected_schema_columns) - set(final_df_columns)\n",
        "extra_columns = set(final_df_columns) - set(expected_schema_columns)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Final Schema Validation Report\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if not missing_columns and not extra_columns:\n",
        "    print(f\"✅ SUCCESS! The final DataFrame schema perfectly matches the expected unified schema.\")\n",
        "    print(f\"Total columns loaded: {len(final_df_columns)}\")\n",
        "else:\n",
        "    print(\"⚠️ ATTENTION: Discrepancy found!\")\n",
        "    if missing_columns:\n",
        "        print(f\"  - Columns defined in schema but MISSING from final DataFrame: {list(missing_columns)}\")\n",
        "    if extra_columns:\n",
        "        print(f\"  - EXTRA columns found in final DataFrame that were not in schema: {list(extra_columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDoZTpido5oT",
        "outputId": "4e9c9d6d-7b74-4c1b-d500-dfd25237e5c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Final Schema Validation Report\n",
            "==================================================\n",
            "✅ SUCCESS! The final DataFrame schema perfectly matches the expected unified schema.\n",
            "Total columns loaded: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import input_file_name, year, col\n",
        "\n",
        "\n",
        "df_bronze_with_metadata = df_bronze_final \\\n",
        "  .withColumn('source_file', input_file_name()) \\\n",
        "  .withColumn('year', year(col('data')))\n",
        "\n",
        "print(\"DataFrame with source_file metadata column:\")\n",
        "df_bronze_with_metadata.select(\"data\", \"bairro\", \"source_file\", \"year\").show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MVHFf6BuFlx",
        "outputId": "5d366c11-b30f-41d9-8dd8-5f3828fa72de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with source_file metadata column:\n",
            "+----------+-----------+---------------------------------------+----+\n",
            "|data      |bairro     |source_file                            |year|\n",
            "+----------+-----------+---------------------------------------+----+\n",
            "|2019-01-01|IPSEP      |file:///content/data/acidentes_2019.csv|2019|\n",
            "|2019-01-01|BOA VIAGEM |file:///content/data/acidentes_2019.csv|2019|\n",
            "|2019-01-01|BOA VIAGEM |file:///content/data/acidentes_2019.csv|2019|\n",
            "|2019-01-01|IMBIRIBEIRA|file:///content/data/acidentes_2019.csv|2019|\n",
            "|2019-01-01|JAQUEIRA   |file:///content/data/acidentes_2019.csv|2019|\n",
            "+----------+-----------+---------------------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Salvando a Tabela Bronze (Saving the Bronze Table)\n",
        "\n",
        "Finalmente, salvamos o DataFrame unificado e enriquecido como uma tabela Delta, que é a base do nosso Lakehouse. A tabela é salva no modo `overwrite` e **particionada por ano** (`partitionBy(\"year\")`) para otimizar drasticamente a performance de futuras consultas que filtrem por data.*texto em itálico*"
      ],
      "metadata": {
        "id": "wwPnej8P7ZeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_table_path = \"delta_lake/bronze/acidentes\"\n",
        "\n",
        "print(\"Saving Bronze table...\")\n",
        "\n",
        "df_bronze_with_metadata.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\") \\\n",
        "    .save(bronze_table_path)\n",
        "\n",
        "print(f\"SUCCESS: Bronze table saved successfully to '{bronze_table_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asGlciWX245_",
        "outputId": "b0cc1049-cb26-4614-9a4b-6e66a9adb39c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Bronze table...\n",
            "SUCCESS: Bronze table saved successfully to 'delta_lake/bronze/acidentes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão da Camada Bronze\n",
        "\n",
        "A Camada Bronze está completa! Nossos dados brutos de múltiplos arquivos inconsistentes estão agora armazenados em uma única tabela Delta, com schema unificado, metadados de linhagem e otimizada para leitura com particionamento.\n",
        "\n",
        "**Próximo Passo:** Iniciar a **Camada Silver**, onde vamos limpar, validar e enriquecer estes dados."
      ],
      "metadata": {
        "id": "8TE3egVP7hwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Camada Silver: Limpeza e Enriquecimento de Dados\n",
        "\n",
        "Iniciamos a Camada Silver, o coração do nosso processo de ETL. O objetivo aqui é transformar os dados brutos e não confiáveis da Camada Bronze em uma fonte de dados limpa, consistente e enriquecida, pronta para análises mais complexas. Aplicaremos regras de negócio e técnicas de limpeza para garantir a **Qualidade e Governança dos Dados**, como defendido por **Reis & Housley** e **Kimball**."
      ],
      "metadata": {
        "id": "sxAbNZl6jB2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Leitura da Tabela Bronze\n",
        "\n",
        "O primeiro passo é carregar nossa tabela Delta da Camada Bronze. Esta ação representa o início do pipeline que move os dados entre as camadas Bronze e Silver."
      ],
      "metadata": {
        "id": "2uIojVnMGvox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_silver = spark.read.format(\"delta\").load(bronze_table_path)\n",
        "\n",
        "print(\"Reading from Bronze Delta table to start the Silver process...\")\n",
        "df_silver.printSchema()\n",
        "print(f\"\\nTotal records read from Bronze: {df_silver.count()}\")\n",
        "df_silver.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJKxCqPojBQN",
        "outputId": "2dda537c-e6ea-449a-a565-02498d0ce7a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading from Bronze Delta table to start the Silver process...\n",
            "root\n",
            " |-- data: date (nullable = true)\n",
            " |-- hora: string (nullable = true)\n",
            " |-- natureza_acidente: string (nullable = true)\n",
            " |-- situacao: string (nullable = true)\n",
            " |-- protocolo: string (nullable = true)\n",
            " |-- bairro: string (nullable = true)\n",
            " |-- endereco: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- detalhe_endereco_acidente: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- endereco_cruzamento: string (nullable = true)\n",
            " |-- numero_cruzamento: string (nullable = true)\n",
            " |-- referencia_cruzamento: string (nullable = true)\n",
            " |-- bairro_cruzamento: string (nullable = true)\n",
            " |-- num_semaforo: string (nullable = true)\n",
            " |-- sentido_via: string (nullable = true)\n",
            " |-- tipo: string (nullable = true)\n",
            " |-- descricao: string (nullable = true)\n",
            " |-- auto: string (nullable = true)\n",
            " |-- moto: string (nullable = true)\n",
            " |-- ciclom: string (nullable = true)\n",
            " |-- ciclista: string (nullable = true)\n",
            " |-- pedestre: string (nullable = true)\n",
            " |-- onibus: string (nullable = true)\n",
            " |-- caminhao: string (nullable = true)\n",
            " |-- viatura: string (nullable = true)\n",
            " |-- outros: string (nullable = true)\n",
            " |-- vitimas: string (nullable = true)\n",
            " |-- vitimas_fatais: string (nullable = true)\n",
            " |-- acidente_verificado: string (nullable = true)\n",
            " |-- tempo_clima: string (nullable = true)\n",
            " |-- situacao_semaforo: string (nullable = true)\n",
            " |-- sinalizacao: string (nullable = true)\n",
            " |-- condicao_via: string (nullable = true)\n",
            " |-- conservacao_via: string (nullable = true)\n",
            " |-- ponto_controle: string (nullable = true)\n",
            " |-- situacao_placa: string (nullable = true)\n",
            " |-- velocidade_max_via: string (nullable = true)\n",
            " |-- mao_direcao: string (nullable = true)\n",
            " |-- divisao_via1: string (nullable = true)\n",
            " |-- divisao_via2: string (nullable = true)\n",
            " |-- divisao_via3: string (nullable = true)\n",
            " |-- source_file: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            "\n",
            "\n",
            "Total records read from Bronze: 30550\n",
            "+----------+-------------------+-----------------+----------+-----------+-----------+---------------------------------+------+-------------------------+---------------------------------------------------------------------------+-------------------+-----------------+---------------------+-----------------+------------+-----------+---------------+---------+----+----+------+--------+--------+------+--------+-------+------+-------+--------------+-------------------+-----------+-----------------+-----------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+---------------------------------------+----+\n",
            "|data      |hora               |natureza_acidente|situacao  |protocolo  |bairro     |endereco                         |numero|detalhe_endereco_acidente|complemento                                                                |endereco_cruzamento|numero_cruzamento|referencia_cruzamento|bairro_cruzamento|num_semaforo|sentido_via|tipo           |descricao|auto|moto|ciclom|ciclista|pedestre|onibus|caminhao|viatura|outros|vitimas|vitimas_fatais|acidente_verificado|tempo_clima|situacao_semaforo|sinalizacao|condicao_via|conservacao_via|ponto_controle|situacao_placa|velocidade_max_via|mao_direcao|divisao_via1|divisao_via2|divisao_via3|source_file                            |year|\n",
            "+----------+-------------------+-----------------+----------+-----------+-----------+---------------------------------+------+-------------------------+---------------------------------------------------------------------------+-------------------+-----------------+---------------------+-----------------+------------+-----------+---------------+---------+----+----+------+--------+--------+------+--------+-------+------+-------+--------------+-------------------+-----------+-----------------+-----------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+---------------------------------------+----+\n",
            "|2025-12-29|2025-10-07 07:09:00|COM VÍTIMA       |FINALIZADA|202435851,0|CASA FORTE |EST DO ENCANAMENTO               |NULL  |NULL                     |EM FRENTE AO COLEGIO JOSE VILELA,  PROX. AO SITIO DA TRINDADE              |NULL               |NULL             |NULL                 |CASA FORTE       |NULL        |NULL       |COLISÃO        |NULL     |0,0 |1,0 |0,0   |0,0     |0,0     |1,0   |0,0     |0,0    |0,0   |2,0    |0,0           |NULL               |NULL       |NULL             |NULL       |NULL        |NULL           |NULL          |NULL          |NULL              |NULL       |NULL        |NULL        |NULL        |file:///content/data/acidentes_2024.csv|2025|\n",
            "|2025-12-29|2025-10-07 07:42:00|VÍTIMA FATAL     |FINALIZADA|202435852,0|BOA VIAGEM |AV ENGENHEIRO DOMINGOS FERREIRA  |NULL  |NULL                     |PROX. AO CLINICAL CENTER,  EM FRENTE AO EMPRESARIAL DOMINGOS FERREIRA      |NULL               |NULL             |NULL                 |BOA VIAGEM       |NULL        |NULL       |COLISÃO        |NULL     |0,0 |1,0 |0,0   |0,0     |0,0     |0,0   |0,0     |0,0    |0,0   |1,0    |2,0           |NULL               |NULL       |NULL             |NULL       |NULL        |NULL           |NULL          |NULL          |NULL              |NULL       |NULL        |NULL        |NULL        |file:///content/data/acidentes_2024.csv|2025|\n",
            "|2025-12-29|2025-10-07 09:29:00|COM VÍTIMA       |FINALIZADA|202435855,0|IMBIRIBEIRA|AV MARECHAL MASCARENHAS DE MORAES|NULL  |NULL                     |EM FRENTE A FERREIRA COSTA , SENTIDO CIDADE                                |NULL               |NULL             |NULL                 |IMBIRIBEIRA      |NULL        |NULL       |COLISÃO LATERAL|NULL     |1,0 |0,0 |0,0   |1,0     |0,0     |0,0   |0,0     |0,0    |0,0   |1,0    |0,0           |NULL               |NULL       |NULL             |NULL       |NULL        |NULL           |NULL          |NULL          |NULL              |NULL       |NULL        |NULL        |NULL        |file:///content/data/acidentes_2024.csv|2025|\n",
            "|2025-12-29|2025-10-07 10:58:00|COM VÍTIMA       |FINALIZADA|202435856,0|DERBY      |AV GOVERNADOR AGAMENON MAGALHAES |NULL  |NULL                     |PROX. AO COLEGIO AMERICANO BATISTA                                         |NULL               |NULL             |NULL                 |DERBY            |NULL        |NULL       |COLISÃO        |NULL     |0,0 |1,0 |0,0   |0,0     |0,0     |0,0   |0,0     |0,0    |0,0   |1,0    |0,0           |NULL               |NULL       |NULL             |NULL       |NULL        |NULL           |NULL          |NULL          |NULL              |NULL       |NULL        |NULL        |NULL        |file:///content/data/acidentes_2024.csv|2025|\n",
            "|2025-12-29|2025-10-07 11:28:00|COM VÍTIMA       |FINALIZADA|202435857,0|SANTO AMARO|AV GOVERNADOR AGAMENON MAGALHAES |SN    |NULL                     |PROX A IGREJA  DO SÉTIMO DIA , PROX AO HOSPITAL INFANTIL HAP VIDA MANDACARU|NULL               |NULL             |NULL                 |SANTO AMARO      |NULL        |NULL       |COLISÃO        |NULL     |0,0 |1,0 |0,0   |0,0     |0,0     |0,0   |0,0     |0,0    |0,0   |1,0    |0,0           |NULL               |NULL       |NULL             |NULL       |NULL        |NULL           |NULL          |NULL          |NULL              |NULL       |NULL        |NULL        |NULL        |file:///content/data/acidentes_2024.csv|2025|\n",
            "+----------+-------------------+-----------------+----------+-----------+-----------+---------------------------------+------+-------------------------+---------------------------------------------------------------------------+-------------------+-----------------+---------------------+-----------------+------------+-----------+---------------+---------+----+----+------+--------+--------+------+--------+-------+------+-------+--------------+-------------------+-----------+-----------------+-----------+------------+---------------+--------------+--------------+------------------+-----------+------------+------------+------------+---------------------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Limpeza e Padronização de Dados (Data Cleaning)\n",
        "\n",
        "Esta etapa foca em melhorar a qualidade e a consistência dos dados, aplicando várias técnicas essenciais de **Limpeza de Dados (McKinney)**.\n"
      ],
      "metadata": {
        "id": "c1gNLp1zkTig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1. Padronização de Case e Tratamento de Nulos\n",
        "\n",
        "Primeiro, padronizamos colunas categóricas para maiúsculas para garantir consistência em agrupamentos e filtros. Em seguida, tratamos valores ausentes em colunas categóricas chave (como `bairro`), substituindo `NULL` por um valor explícito (`'NAO INFORMADO'`), melhorando a usabilidade dos dados conforme as práticas de **Qualidade de Dados (Kimball)**."
      ],
      "metadata": {
        "id": "Mlibnggap9Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper, col\n",
        "\n",
        "columns_to_upper = ['natureza_acidente', 'situacao', 'bairro', 'endereco', 'bairro_cruzamento', 'sentido_via', 'tipo', 'tempo_clima', 'mao_direcao']\n",
        "\n",
        "df_silver_casing = df_silver\n",
        "for col_name in columns_to_upper:\n",
        "  df_silver_casing = df_silver_casing.withColumn(col_name, upper(col(col_name)))\n",
        "\n",
        "print(\"Verification of the loop transformation:\")\n",
        "df_silver_casing.select(columns_to_upper).show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC0-PwPEotxC",
        "outputId": "4aa36319-72f8-4b69-8bf5-4b7188a25b88"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification of the loop transformation:\n",
            "+-----------------+----------+-----------+---------------------------------+-----------------+-----------+---------------+-----------+-----------+\n",
            "|natureza_acidente|situacao  |bairro     |endereco                         |bairro_cruzamento|sentido_via|tipo           |tempo_clima|mao_direcao|\n",
            "+-----------------+----------+-----------+---------------------------------+-----------------+-----------+---------------+-----------+-----------+\n",
            "|COM VÍTIMA       |FINALIZADA|CASA FORTE |EST DO ENCANAMENTO               |CASA FORTE       |NULL       |COLISÃO        |NULL       |NULL       |\n",
            "|VÍTIMA FATAL     |FINALIZADA|BOA VIAGEM |AV ENGENHEIRO DOMINGOS FERREIRA  |BOA VIAGEM       |NULL       |COLISÃO        |NULL       |NULL       |\n",
            "|COM VÍTIMA       |FINALIZADA|IMBIRIBEIRA|AV MARECHAL MASCARENHAS DE MORAES|IMBIRIBEIRA      |NULL       |COLISÃO LATERAL|NULL       |NULL       |\n",
            "|COM VÍTIMA       |FINALIZADA|DERBY      |AV GOVERNADOR AGAMENON MAGALHAES |DERBY            |NULL       |COLISÃO        |NULL       |NULL       |\n",
            "|COM VÍTIMA       |FINALIZADA|SANTO AMARO|AV GOVERNADOR AGAMENON MAGALHAES |SANTO AMARO      |NULL       |COLISÃO        |NULL       |NULL       |\n",
            "+-----------------+----------+-----------+---------------------------------+-----------------+-----------+---------------+-----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset = ['natureza_acidente', 'situacao', 'bairro', 'sentido_via', 'tipo']\n",
        "\n",
        "df_silver_nulls_handled = df_silver_casing.na.fill('NÃO INFORMADO', subset)\n",
        "\n",
        "print(\"\\nVerification of null handling:\")\n",
        "df_silver_nulls_handled.filter(col('situacao') == 'NÃO INFORMADO').select(subset).show(5, truncate=False)"
      ],
      "metadata": {
        "id": "ArUGUCblwra7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c2e1bf-4a65-4fd3-b13b-3e29bf47fd97"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verification of null handling:\n",
            "+-----------------+-------------+------------------+-------------+-------------------------+\n",
            "|natureza_acidente|situacao     |bairro            |sentido_via  |tipo                     |\n",
            "+-----------------+-------------+------------------+-------------+-------------------------+\n",
            "|NÃO INFORMADO    |NÃO INFORMADO|BOMBA DO HEMETÉRIO|CIDADE       |CHOQUE OBJETO FIXO       |\n",
            "|NÃO INFORMADO    |NÃO INFORMADO|SANTO ANTÔNIO     |SUBURBIO     |ABALROAMENTO LONGITUDINAL|\n",
            "|NÃO INFORMADO    |NÃO INFORMADO|IPUTINGA          |CIDADE       |ABALROAMENTO LONGITUDINAL|\n",
            "|NÃO INFORMADO    |NÃO INFORMADO|IPUTINGA          |CIDADE       |ABALROAMENTO LONGITUDINAL|\n",
            "|NÃO INFORMADO    |NÃO INFORMADO|AFOGADOS          |NÃO INFORMADO|COLISÃO                  |\n",
            "+-----------------+-------------+------------------+-------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2. Correção de Tipos de Dados Numéricos\n",
        "\n",
        "Durante a implementação das validações, notamos um comportamento inesperado: linhas com contagens válidas (ex: 1 ou 2) estavam sendo removidas. A investigação revelou que as colunas numéricas de contagem foram lidas como `StringType` na Camada Bronze, devido a valores formatados com vírgula (ex: `'1,0'`) em alguns dos arquivos de origem.\n",
        "\n",
        "Para corrigir isso, aplicamos uma transformação em duas etapas:\n",
        "1.  **Substituímos** a vírgula por ponto (`regexp_replace`).\n",
        "2.  **Convertemos** a coluna para o tipo `IntegerType` (`.cast()`).\n",
        "\n",
        "Este passo é um exemplo prático de depuração e correção de tipos de dados em um pipeline real."
      ],
      "metadata": {
        "id": "x3InbHirqJhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, regexp_replace\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_to_fix = df_silver_nulls_handled\n",
        "counter_columns = [\"auto\", \"moto\", \"ciclom\", \"ciclista\", \"pedestre\", \"onibus\",\"caminhao\", \"viatura\", \"outros\", \"vitimas\", \"vitimas_fatais\"]\n",
        "\n",
        "print(\"--- Correcting data types for counter columns ---\")\n",
        "\n",
        "for col_name in counter_columns:\n",
        "  df_to_fix = df_to_fix.withColumn(col_name, regexp_replace(col(col_name),',','.').cast(IntegerType()))\n",
        "\n",
        "df_silver_types_fixed = df_to_fix\n",
        "\n",
        "print(\"\\nSchema after data type correction:\")\n",
        "df_silver_types_fixed.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww5fjpfDfXvv",
        "outputId": "3a4b75bd-ebba-46cb-9218-9da5ae4637c4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Correcting data types for counter columns ---\n",
            "\n",
            "Schema after data type correction:\n",
            "root\n",
            " |-- data: date (nullable = true)\n",
            " |-- hora: string (nullable = true)\n",
            " |-- natureza_acidente: string (nullable = false)\n",
            " |-- situacao: string (nullable = false)\n",
            " |-- protocolo: string (nullable = true)\n",
            " |-- bairro: string (nullable = false)\n",
            " |-- endereco: string (nullable = true)\n",
            " |-- numero: string (nullable = true)\n",
            " |-- detalhe_endereco_acidente: string (nullable = true)\n",
            " |-- complemento: string (nullable = true)\n",
            " |-- endereco_cruzamento: string (nullable = true)\n",
            " |-- numero_cruzamento: string (nullable = true)\n",
            " |-- referencia_cruzamento: string (nullable = true)\n",
            " |-- bairro_cruzamento: string (nullable = true)\n",
            " |-- num_semaforo: string (nullable = true)\n",
            " |-- sentido_via: string (nullable = false)\n",
            " |-- tipo: string (nullable = false)\n",
            " |-- descricao: string (nullable = true)\n",
            " |-- auto: integer (nullable = true)\n",
            " |-- moto: integer (nullable = true)\n",
            " |-- ciclom: integer (nullable = true)\n",
            " |-- ciclista: integer (nullable = true)\n",
            " |-- pedestre: integer (nullable = true)\n",
            " |-- onibus: integer (nullable = true)\n",
            " |-- caminhao: integer (nullable = true)\n",
            " |-- viatura: integer (nullable = true)\n",
            " |-- outros: integer (nullable = true)\n",
            " |-- vitimas: integer (nullable = true)\n",
            " |-- vitimas_fatais: integer (nullable = true)\n",
            " |-- acidente_verificado: string (nullable = true)\n",
            " |-- tempo_clima: string (nullable = true)\n",
            " |-- situacao_semaforo: string (nullable = true)\n",
            " |-- sinalizacao: string (nullable = true)\n",
            " |-- condicao_via: string (nullable = true)\n",
            " |-- conservacao_via: string (nullable = true)\n",
            " |-- ponto_controle: string (nullable = true)\n",
            " |-- situacao_placa: string (nullable = true)\n",
            " |-- velocidade_max_via: string (nullable = true)\n",
            " |-- mao_direcao: string (nullable = true)\n",
            " |-- divisao_via1: string (nullable = true)\n",
            " |-- divisao_via2: string (nullable = true)\n",
            " |-- divisao_via3: string (nullable = true)\n",
            " |-- source_file: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Enriquecimento de Dados (Feature Engineering)\n",
        "\n",
        "Nesta fase de **Transformação e Mapeamento (McKinney)**, criamos novas colunas (*features*) que agregam valor analítico e facilitam as consultas.\n",
        "\n",
        "* **Atributos de Data:** A partir da coluna `data`, derivamos `month`, `day_of_month` e `day_of_week`. Este é um passo fundamental na criação de uma futura **Dimensão Calendário (Kimball)**.\n",
        "* **Atributos de Tempo:** Usando lógica condicional (`when/otherwise`), transformamos a coluna `hora` em uma categoria `periodo_do_dia`, o que simplifica análises baseadas em períodos."
      ],
      "metadata": {
        "id": "S_F4r8CPrZCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month, col, date_format, day\n",
        "\n",
        "df_silver_enriched = df_silver_types_fixed.withColumn('day_of_month', day(col('data'))) \\\n",
        "  .withColumn('month', month(col('data'))) \\\n",
        "  .withColumn('day_of_week', date_format(col('data'), 'E'))\n",
        "\n",
        "print(\"Verification of the new 'columns' column:\")\n",
        "df_silver_enriched.select(\"data\", \"month\", \"day_of_week\", \"day_of_month\").show(5, truncate=False)"
      ],
      "metadata": {
        "id": "fOcnFsVdy8Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ea7f4a-b696-48ae-82ae-df71c45994d8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification of the new 'columns' column:\n",
            "+----------+-----+-----------+------------+\n",
            "|data      |month|day_of_week|day_of_month|\n",
            "+----------+-----+-----------+------------+\n",
            "|2025-12-29|12   |Mon        |29          |\n",
            "|2025-12-29|12   |Mon        |29          |\n",
            "|2025-12-29|12   |Mon        |29          |\n",
            "|2025-12-29|12   |Mon        |29          |\n",
            "|2025-12-29|12   |Mon        |29          |\n",
            "+----------+-----+-----------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, hour, when\n",
        "\n",
        "df_silver_final = df_silver_enriched.withColumn(\"periodo_do_dia\",\n",
        "  when((hour(col(\"hora\")) >= 6) & (hour(col(\"hora\")) < 12), \"MANHA\") \\\n",
        "  .when((hour(col(\"hora\")) >= 12) & (hour(col(\"hora\")) < 18), \"TARDE\") \\\n",
        "  .when((hour(col(\"hora\")) >= 18) & (hour(col(\"hora\")) <= 23), \"NOITE\") \\\n",
        "  .otherwise(\"MADRUGADA\")\n",
        ")\n",
        "\n",
        "print(\"Verification of the new 'periodo_do_dia' column:\")\n",
        "df_silver_final.groupBy(\"periodo_do_dia\").count().orderBy(\"count\", ascending=False).show(truncate=False)"
      ],
      "metadata": {
        "id": "7KQRrRtb6hMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b70d64-48da-4f78-e015-a4e2907325a1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification of the new 'periodo_do_dia' column:\n",
            "+--------------+-----+\n",
            "|periodo_do_dia|count|\n",
            "+--------------+-----+\n",
            "|MANHA         |11889|\n",
            "|TARDE         |8046 |\n",
            "|MADRUGADA     |6565 |\n",
            "|NOITE         |4050 |\n",
            "+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Validação e Garantia de Qualidade (Data Quality Assurance)\n",
        "\n",
        "Com os dados limpos e enriquecidos, realizamos as validações finais para garantir a integridade da tabela Silver antes de salvá-la.\n",
        "\n",
        "#### 3.4.1. Remoção de Duplicatas\n",
        "\n",
        "Removemos quaisquer linhas que sejam 100% idênticas em todas as colunas usando `.dropDuplicates()`. Este passo garante a unicidade dos registros e evita métricas infladas."
      ],
      "metadata": {
        "id": "dA-RpvtFrUVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_count = df_silver_final.count()\n",
        "print(f\"Row count before deduplication: {initial_count}\")\n",
        "\n",
        "df_silver_deduplicated = df_silver_final.dropDuplicates()\n",
        "\n",
        "final_count = df_silver_deduplicated.count()\n",
        "print(f\"Row count after deduplication: {final_count}\")\n",
        "print(f\"Number of duplicate rows removed: {initial_count - final_count}\")\n",
        "\n",
        "if initial_count == final_count:\n",
        "  print(\"\\nResult: No duplicate rows were found.\")\n",
        "else:\n",
        "  print(\"\\nResult: Duplicate rows were successfully removed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1MpGzYXcwdo",
        "outputId": "8918bcda-02d2-4759-e843-24d50d4c1e10"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row count before deduplication: 30550\n",
            "Row count after deduplication: 30549\n",
            "Number of duplicate rows removed: 1\n",
            "\n",
            "Result: Duplicate rows were successfully removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4.2. Validação de Regras de Negócio\n",
        "\n",
        "Finalmente, após corrigir os tipos de dados, aplicamos a regra de negócio de que nenhuma coluna de contagem pode ser negativa. O filtro confirmou que não haviam registros com valores negativos, validando a qualidade dos dados neste quesito e mostrando que a origem, apesar dos problemas de formato, não continha dados logicamente inválidos."
      ],
      "metadata": {
        "id": "a9j_XFbjrZQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "print(\"--- Running the filtering logic ---\")\n",
        "df_filtered = df_silver_deduplicated\n",
        "for c in counter_columns:\n",
        "  df_filtered = df_filtered.filter((col(c) >= 0) | (col(c).isNull()))\n",
        "\n",
        "removed_rows_df = df_silver_deduplicated.subtract(df_filtered)\n",
        "\n",
        "print(f\"\\nTotal rows removed: {removed_rows_df.count()}\")\n",
        "print(\"\\n--- Inspecting a sample of the REMOVED rows ---\")\n",
        "print(\"Showing the values in the counter columns for the rows that were filtered out:\")\n",
        "\n",
        "removed_rows_df.select(counter_columns).show(20, truncate=False)\n",
        "\n",
        "df_silver_final_validated = df_filtered"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8dQgzm5q9wW",
        "outputId": "34589399-910e-431d-f436-6f364ce23f23"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running the filtering logic ---\n",
            "\n",
            "Total rows removed: 0\n",
            "\n",
            "--- Inspecting a sample of the REMOVED rows ---\n",
            "Showing the values in the counter columns for the rows that were filtered out:\n",
            "+----+----+------+--------+--------+------+--------+-------+------+-------+--------------+\n",
            "|auto|moto|ciclom|ciclista|pedestre|onibus|caminhao|viatura|outros|vitimas|vitimas_fatais|\n",
            "+----+----+------+--------+--------+------+--------+-------+------+-------+--------------+\n",
            "+----+----+------+--------+--------+------+--------+-------+------+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Salvando a Tabela Silver (Saving the Silver Table)\n",
        "\n",
        "Como passo final da Etapa 2, persistimos nosso DataFrame limpo, enriquecido e validado. Ao salvá-lo como uma nova tabela Delta, materializamos o resultado do nosso pipeline Bronze-to-Silver.\n",
        "\n",
        "Esta tabela Silver servirá como uma fonte da verdade confiável e performática para as próximas etapas de análise e agregação na Camada Gold. Mantemos o particionamento por `year` para garantir que as consultas continuem otimizadas."
      ],
      "metadata": {
        "id": "Pr7bZ_OAvmSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_save = df_silver_final_validated\n",
        "\n",
        "silver_table_path = \"delta_lake/silver/acidentes_limpos\"\n",
        "\n",
        "print(\"Saving Silver table...\")\n",
        "\n",
        "df_to_save.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\") \\\n",
        "    .save(silver_table_path)\n",
        "\n",
        "print(f\"SUCCESS: Silver table saved successfully to '{silver_table_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQkLW5ccvOP5",
        "outputId": "8410aced-2947-4eb1-809b-0f30cc765659"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Silver table...\n",
            "SUCCESS: Silver table saved successfully to 'delta_lake/silver/acidentes_limpos'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão da Camada Silver\n",
        "\n",
        "A Camada Silver está completa! Transformamos os dados brutos e inconsistentes da Camada Bronze em um conjunto de dados de alta qualidade. Durante este processo, nós:\n",
        "\n",
        "* **Limpamos e Padronizamos** os dados, tratando valores nulos e garantindo a consistência de colunas categóricas.\n",
        "* **Corrigimos Tipos de Dados**, resolvendo um problema sutil de formato nos dados de origem.\n",
        "* **Enriquecemos** o dataset com novos atributos derivados de data e hora, agregando valor analítico.\n",
        "* **Validamos** as regras de negócio, removendo duplicatas e garantindo a integridade lógica dos dados.\n",
        "\n",
        "O resultado é uma tabela Delta (`acidentes_limpos`) que representa a \"fonte única da verdade\" para os acidentes de trânsito do Recife, pronta para ser consumida pela Camada Gold.\n",
        "\n",
        "**Próximo Passo:** Iniciar a **Camada Gold**, onde construiremos agregações e Data Marts específicos para responder a perguntas de negócio."
      ],
      "metadata": {
        "id": "ac3zX-Ckvpz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Camada Gold: Modelagem Dimensional para Análise\n",
        "\n",
        "Finalmente, na Camada Gold, transformamos os dados limpos da Silver em modelos otimizados para consumo por analistas e ferramentas de BI. O objetivo é criar \"Produtos de Dados\" que respondam a perguntas de negócio de forma rápida e intuitiva. Para isso, implementaremos um **Modelo Dimensional** no estilo **Star Schema (Kimball)**."
      ],
      "metadata": {
        "id": "M7U8jn7xxGXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_gold = spark.read.format(\"delta\").load(silver_table_path)"
      ],
      "metadata": {
        "id": "oDlNSjD3xJY_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Construção das Tabelas de Dimensão\n",
        "\n",
        "O primeiro passo na criação de um Star Schema é construir as Tabelas de Dimensão. Elas fornecem o contexto descritivo (\"quem, o quê, onde, quando\") para as nossas métricas. Para cada dimensão, selecionamos os atributos únicos da tabela Silver e geramos uma **chave substituta** (*surrogate key*) numérica para garantir a integridade e a performance dos `JOIN`s."
      ],
      "metadata": {
        "id": "sT4QeBc-CGM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1. Dim_TipoAcidente\n",
        "Esta dimensão cataloga as combinações únicas de `natureza_acidente` e `tipo` de acidente."
      ],
      "metadata": {
        "id": "tEGMzRZqXmxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "acidente_columns = ['tipo','natureza_acidente']\n",
        "dim_tipo_acidente_base = df_gold.select(acidente_columns).distinct()\n",
        "windowSpec = Window.orderBy(acidente_columns)\n",
        "dim_tipo_acidente = dim_tipo_acidente_base.withColumn('tipo_acidente_key', row_number().over(windowSpec))"
      ],
      "metadata": {
        "id": "m24fgpPhCQYN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim_tipo_acidente.select(\"tipo_acidente_key\", \"natureza_acidente\", \"tipo\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xvftpK0J9YN",
        "outputId": "c6bcda0c-7636-4076-ef16-da6ab5179e62"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+-------------------------+\n",
            "|tipo_acidente_key|natureza_acidente|tipo                     |\n",
            "+-----------------+-----------------+-------------------------+\n",
            "|1                |COM VÍTIMA       |ABALROAMENTO LONGITUDINAL|\n",
            "|2                |NÃO INFORMADO    |ABALROAMENTO LONGITUDINAL|\n",
            "|3                |SEM VÍTIMA       |ABALROAMENTO LONGITUDINAL|\n",
            "|4                |VÍTIMA FATAL     |ABALROAMENTO LONGITUDINAL|\n",
            "|5                |COM VÍTIMA       |ABALROAMENTO TRANSVERSAL |\n",
            "|6                |SEM VÍTIMA       |ABALROAMENTO TRANSVERSAL |\n",
            "|7                |VÍTIMA FATAL     |ABALROAMENTO TRANSVERSAL |\n",
            "|8                |COM VÍTIMA       |ACID. DE PERCURSO        |\n",
            "|9                |SEM VÍTIMA       |ACID. DE PERCURSO        |\n",
            "|10               |SEM VÍTIMA       |ALAGAMENTO               |\n",
            "|11               |SEM VÍTIMA       |APOIO CELPE              |\n",
            "|12               |SEM VÍTIMA       |APOIO COMPESA            |\n",
            "|13               |COM VÍTIMA       |APOIO EMLURB             |\n",
            "|14               |COM VÍTIMA       |ATROPELAMENTO            |\n",
            "|15               |SEM VÍTIMA       |ATROPELAMENTO            |\n",
            "|16               |VÍTIMA FATAL     |ATROPELAMENTO            |\n",
            "|17               |COM VÍTIMA       |ATROPELAMENTO ANIMAL     |\n",
            "|18               |SEM VÍTIMA       |ATROPELAMENTO ANIMAL     |\n",
            "|19               |COM VÍTIMA       |ATROPELAMENTO DE ANIMAL  |\n",
            "|20               |SEM VÍTIMA       |ATROPELAMENTO DE ANIMAL  |\n",
            "+-----------------+-----------------+-------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.2. Dim_Localizacao\n",
        "Esta dimensão cataloga as combinações únicas de `bairro`, `endereco` e `sentido_via`, representando as diferentes localidades dos acidentes."
      ],
      "metadata": {
        "id": "xwXNVpwtKZXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "localizacao_columns = ['bairro','endereco','sentido_via']\n",
        "\n",
        "dim_localizacao_base = df_gold.select(localizacao_columns).distinct()\n",
        "windowSpec = Window.orderBy(localizacao_columns)\n",
        "dim_localizacao = dim_localizacao_base.withColumn('localizacao_key', row_number().over(windowSpec))"
      ],
      "metadata": {
        "id": "zAajvT_ZKahR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim_localizacao.select('*').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xScTlxQYLelU",
        "outputId": "01ac1ac8-3ed2-4154-e684-dbafd9e8982f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-------------+---------------+\n",
            "| bairro|            endereco|  sentido_via|localizacao_key|\n",
            "+-------+--------------------+-------------+---------------+\n",
            "|      0|AV NORTE MIGUEL A...|NÃO INFORMADO|              1|\n",
            "|AFLITOS|AV CONS ROSA E SILVA|NÃO INFORMADO|              2|\n",
            "|AFLITOS|AV CONS ROSA E SILVA|     SUBURBIO|              3|\n",
            "|AFLITOS|            AV NORTE|NÃO INFORMADO|              4|\n",
            "|AFLITOS|      AV RUI BARBOSA|NÃO INFORMADO|              5|\n",
            "|AFLITOS|    AV SANTOS DUMONT|       CIDADE|              6|\n",
            "|AFLITOS|    AV SANTOS DUMONT|NÃO INFORMADO|              7|\n",
            "|AFLITOS|    AV SANTOS DUMONT|     SUBURBIO|              8|\n",
            "|AFLITOS|       CAIS DO APOLO|     SUBURBIO|              9|\n",
            "|AFLITOS|       RUA ANGUSTURA|NÃO INFORMADO|             10|\n",
            "|AFLITOS|RUA CAPITAO SAMPA...|     SUBURBIO|             11|\n",
            "|AFLITOS| RUA CARNEIRO VILELA|NÃO INFORMADO|             12|\n",
            "|AFLITOS| RUA CARNEIRO VILELA|     SUBURBIO|             13|\n",
            "|AFLITOS|    RUA CONS PORTELA|       CIDADE|             14|\n",
            "|AFLITOS|    RUA CONS PORTELA|NÃO INFORMADO|             15|\n",
            "|AFLITOS|    RUA CONS PORTELA|     SUBURBIO|             16|\n",
            "|AFLITOS|    RUA DAS CREOULAS|       CIDADE|             17|\n",
            "|AFLITOS|RUA DES MARTINS P...|NÃO INFORMADO|             18|\n",
            "|AFLITOS|       RUA DO FUTURO|NÃO INFORMADO|             19|\n",
            "|AFLITOS|       RUA DO FUTURO|     SUBURBIO|             20|\n",
            "+-------+--------------------+-------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.3. Dim_Tempo\n",
        "Esta é uma dimensão calendário clássica, contendo uma linha para cada dia único em que ocorreu um acidente. Criamos uma chave `data_key` no formato `YYYYMMDD` para otimização de `JOIN`s."
      ],
      "metadata": {
        "id": "WhAzEHq2NHdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tempo_columns = ['data', 'year', 'month', 'day_of_month', 'day_of_week']\n",
        "dim_tempo = df_gold.select(\n",
        "  date_format(col('data'), 'yyyyMMdd').cast(IntegerType()).alias('tempo_key'),\n",
        "  *tempo_columns\n",
        " ).distinct().orderBy('tempo_key')"
      ],
      "metadata": {
        "id": "4b3b8YsxNIxf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim_tempo.printSchema()\n",
        "dim_tempo.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Y1sDTdQTTx",
        "outputId": "f90ce1a1-aea2-4218-ee99-7f152ff47871"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- tempo_key: integer (nullable = true)\n",
            " |-- data: date (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day_of_month: integer (nullable = true)\n",
            " |-- day_of_week: string (nullable = true)\n",
            "\n",
            "+---------+----------+----+-----+------------+-----------+\n",
            "|tempo_key|      data|year|month|day_of_month|day_of_week|\n",
            "+---------+----------+----+-----+------------+-----------+\n",
            "| 20190101|2019-01-01|2019|    1|           1|        Tue|\n",
            "| 20190102|2019-01-02|2019|    1|           2|        Wed|\n",
            "| 20190103|2019-01-03|2019|    1|           3|        Thu|\n",
            "| 20190104|2019-01-04|2019|    1|           4|        Fri|\n",
            "| 20190105|2019-01-05|2019|    1|           5|        Sat|\n",
            "| 20190106|2019-01-06|2019|    1|           6|        Sun|\n",
            "| 20190107|2019-01-07|2019|    1|           7|        Mon|\n",
            "| 20190108|2019-01-08|2019|    1|           8|        Tue|\n",
            "| 20190109|2019-01-09|2019|    1|           9|        Wed|\n",
            "| 20190110|2019-01-10|2019|    1|          10|        Thu|\n",
            "+---------+----------+----+-----+------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Construção da Tabela Fato\n",
        "\n",
        "Com as dimensões prontas, construímos a Tabela Fato (`fato_acidentes`). Esta é a tabela central do nosso modelo, contendo:\n",
        "- **Chaves Estrangeiras:** As chaves (`data_key`, `localizacao_key`, `tipo_acidente_key`) que se conectam às nossas dimensões.\n",
        "- **Métricas (Fatos):** Os valores numéricos que queremos analisar (`quantidade_vitimas`, `total_veiculos_envolvidos`, etc.).\n",
        "\n",
        "Construímos a tabela juntando (`JOIN`) a tabela Silver com cada uma das dimensões para buscar as chaves e, em seguida, selecionamos e calculamos as colunas de fatos."
      ],
      "metadata": {
        "id": "I0nVMn7TkeOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, coalesce, expr\n",
        "\n",
        "vehicle_cols = [\"auto\", \"moto\", \"ciclom\", \"ciclista\", \"pedestre\", \"onibus\",\"caminhao\", \"viatura\", \"outros\"]\n",
        "\n",
        "sum_expr_str = \" + \".join([f\"coalesce({c}, 0)\" for c in vehicle_cols])\n",
        "\n",
        "df_fact_acidentes_joined = df_gold.join(dim_tempo, on='data', how='left') \\\n",
        "  .join(dim_localizacao, on=localizacao_columns, how='left') \\\n",
        "  .join(dim_tipo_acidente, on=acidente_columns, how='left')\n",
        "fact_acidentes = df_fact_acidentes_joined.withColumn('contagem_acidentes', lit(1)) \\\n",
        "  .withColumn('total_veiculos_envolvidos', expr(sum_expr_str)) \\\n",
        "  .select('tempo_key', 'localizacao_key', 'tipo_acidente_key',\n",
        "          coalesce(col('vitimas'), lit(0)).alias('quantidade_vitimas'),\n",
        "          coalesce(col('vitimas_fatais'), lit(0)).alias('quantidade_vitimas_fatais'),\n",
        "          'contagem_acidentes',\n",
        "          'total_veiculos_envolvidos'\n",
        "          )\n",
        "\n",
        "print(f\"Count before join: {df_gold.count()}\")\n",
        "print(f\"Count after join:  {fact_acidentes.count()}\")\n",
        "\n",
        "fact_acidentes.printSchema()\n",
        "print('='*100)\n",
        "fact_acidentes.show(10)"
      ],
      "metadata": {
        "id": "tDZfgPi3RqVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8860ffd-0e86-4599-dba0-216878040d23"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count before join: 30549\n",
            "Count after join:  30549\n",
            "root\n",
            " |-- tempo_key: integer (nullable = true)\n",
            " |-- localizacao_key: integer (nullable = true)\n",
            " |-- tipo_acidente_key: integer (nullable = true)\n",
            " |-- quantidade_vitimas: integer (nullable = false)\n",
            " |-- quantidade_vitimas_fatais: integer (nullable = false)\n",
            " |-- contagem_acidentes: integer (nullable = false)\n",
            " |-- total_veiculos_envolvidos: integer (nullable = false)\n",
            "\n",
            "====================================================================================================\n",
            "+---------+---------------+-----------------+------------------+-------------------------+------------------+-------------------------+\n",
            "|tempo_key|localizacao_key|tipo_acidente_key|quantidade_vitimas|quantidade_vitimas_fatais|contagem_acidentes|total_veiculos_envolvidos|\n",
            "+---------+---------------+-----------------+------------------+-------------------------+------------------+-------------------------+\n",
            "| 20200115|           2437|               34|                 0|                        0|                 1|                        2|\n",
            "| 20200120|           2097|               45|                 0|                        0|                 1|                        2|\n",
            "| 20200206|           3592|                3|                 0|                        0|                 1|                        2|\n",
            "| 20200211|           1358|               39|                 0|                        0|                 1|                        2|\n",
            "| 20200213|            127|               39|                 0|                        0|                 1|                        3|\n",
            "| 20200221|           2326|                5|                 1|                        0|                 1|                        2|\n",
            "| 20200223|           5212|               39|                 0|                        0|                 1|                        2|\n",
            "| 20200227|           2158|               54|                 0|                        0|                 1|                        3|\n",
            "| 20200310|           2047|                5|                 1|                        0|                 1|                        2|\n",
            "| 20200311|           5012|               27|                 0|                        0|                 1|                        1|\n",
            "+---------+---------------+-----------------+------------------+-------------------------+------------------+-------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Salvando as Tabelas do Data Mart\n",
        "\n",
        "Como passo final, persistimos todas as tabelas do nosso Star Schema (3 Dimensões e 1 Fato) como tabelas Delta na camada Gold. Isso as materializa em disco, prontas para serem lidas por qualquer ferramenta de análise."
      ],
      "metadata": {
        "id": "azobENBznTVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_tables_to_save = {\n",
        "    \"dim_tipo_acidente\": dim_tipo_acidente,\n",
        "    \"dim_localizacao\": dim_localizacao,\n",
        "    \"dim_tempo\": dim_tempo,\n",
        "    \"fact_acidentes\": fact_acidentes\n",
        "}\n",
        "for table_name, df_object in gold_tables_to_save.items():\n",
        "  gold_table_path = f\"delta_lake/gold/{table_name}\"\n",
        "  df_object.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(gold_table_path)\n",
        "\n",
        "  print(f\"Saving '{table_name}' table to '{gold_table_path}'...\")\n",
        "\n",
        "print(f\"SUCCESS: All Gold tables saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lssEppmeki7b",
        "outputId": "4b1e472e-4b06-48b4-8015-7bf0257c5fd7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 'dim_tipo_acidente' table to 'delta_lake/gold/dim_tipo_acidente'...\n",
            "Saving 'dim_localizacao' table to 'delta_lake/gold/dim_localizacao'...\n",
            "Saving 'dim_tempo' table to 'delta_lake/gold/dim_tempo'...\n",
            "Saving 'fact_acidentes' table to 'delta_lake/gold/fact_acidentes'...\n",
            "SUCCESS: All Gold tables saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Data Mart 2: Análise de Veículos Envolvidos (Modelo Unpivot)\n",
        "\n",
        "Para responder a perguntas mais específicas sobre os veículos, construímos um segundo Data Mart com uma abordagem de modelagem diferente. O objetivo aqui é analisar os dados pela ótica dos veículos, e não apenas dos acidentes.\n",
        "\n",
        "Para isso, aplicamos uma transformação avançada de **Unpivot** (usando a função `stack`) para mudar a granularidade dos dados de \"um registro por acidente\" para \"um registro por tipo de veículo por acidente\".\n",
        "\n",
        "Este modelo, que resolve uma relação muitos-para-muitos através de uma tabela fato de granularidade fina, nos permite analisar diretamente quais veículos estão mais envolvidos e em que contextos, demonstrando uma técnica de ETL mais complexa e poderosa. O processo envolveu:\n",
        "1.  Criação de uma nova dimensão, `Dim_Vehicle`.\n",
        "2.  Aplicação da transformação `unpivot` na tabela Silver.\n",
        "3.  Criação de uma nova tabela Fato (`Fato_Acidentes_Veiculos`) juntando o resultado do unpivot com todas as dimensões."
      ],
      "metadata": {
        "id": "KrqVTYzvf0A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_cols = ['auto', 'moto', 'ciclom', 'ciclista', 'pedestre', 'onibus', 'caminhao', 'viatura', 'outros']\n",
        "vehicle_rows = [(vehicle, ) for vehicle in vehicle_cols]\n",
        "dim_vehicle_base = spark.createDataFrame(vehicle_rows, ['vehicle_type'],)\n",
        "\n",
        "windowSpec = Window.orderBy('vehicle_type')\n",
        "dim_vehicle = dim_vehicle_base.withColumn('vehicle_key', row_number().over(windowSpec))\n",
        "\n",
        "print(\"\\n Dimension Vehicle\")\n",
        "dim_vehicle.show()\n",
        "\n",
        "n_cols = len(vehicle_cols)\n",
        "args_str = \", \".join([f\"'{c}', {c}\" for c in vehicle_cols])\n",
        "formula_pivot = f\"stack({n_cols}, {args_str}) as (vehicle_type, quantity)\"\n",
        "\n",
        "df_unpivoted = df_gold.select(\n",
        "    \"data\",\n",
        "    \"bairro\",\n",
        "    \"endereco\",\n",
        "    \"sentido_via\",\n",
        "    \"natureza_acidente\",\n",
        "    \"tipo\",\n",
        "    expr(formula_pivot)\n",
        ").filter(col(\"quantity\") > 0)\n",
        "\n",
        "print(\"\\n Unpivoted Table\")\n",
        "df_unpivoted.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1UTX2BIf1FR",
        "outputId": "00b05cf9-a670-4d54-a83e-51abf17e364b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dimension Vehicle\n",
            "+------------+-----------+\n",
            "|vehicle_type|vehicle_key|\n",
            "+------------+-----------+\n",
            "|        auto|          1|\n",
            "|    caminhao|          2|\n",
            "|    ciclista|          3|\n",
            "|      ciclom|          4|\n",
            "|        moto|          5|\n",
            "|      onibus|          6|\n",
            "|      outros|          7|\n",
            "|    pedestre|          8|\n",
            "|     viatura|          9|\n",
            "+------------+-----------+\n",
            "\n",
            "\n",
            " Unpivoted Table\n",
            "+----------+-----------+--------------------+-------------+-----------------+--------------------+------------+--------+\n",
            "|      data|     bairro|            endereco|  sentido_via|natureza_acidente|                tipo|vehicle_type|quantity|\n",
            "+----------+-----------+--------------------+-------------+-----------------+--------------------+------------+--------+\n",
            "|2019-02-02| CASA FORTE|AV DEZESSETE DE A...|     SUBURBIO|       COM VÍTIMA|ABALROAMENTO TRAN...|        auto|       1|\n",
            "|2019-02-02| CASA FORTE|AV DEZESSETE DE A...|     SUBURBIO|       COM VÍTIMA|ABALROAMENTO TRAN...|        moto|       1|\n",
            "|2019-02-03| BOA VIAGEM|RUA PADRE BERNADI...|     SUBURBIO|       SEM VÍTIMA|ABALROAMENTO TRAN...|        auto|       2|\n",
            "|2019-02-06| CASA FORTE|   RUA EDSON ALVARES|NÃO INFORMADO|       SEM VÍTIMA|ABALROAMENTO LONG...|        auto|       1|\n",
            "|2019-02-06| CASA FORTE|   RUA EDSON ALVARES|NÃO INFORMADO|       SEM VÍTIMA|ABALROAMENTO LONG...|    caminhao|       1|\n",
            "|2019-02-12| BOA VIAGEM|RUA DR LUIZ CORRE...|     SUBURBIO|       COM VÍTIMA|ABALROAMENTO LONG...|        auto|       1|\n",
            "|2019-02-12| BOA VIAGEM|RUA DR LUIZ CORRE...|     SUBURBIO|       COM VÍTIMA|ABALROAMENTO LONG...|        moto|       1|\n",
            "|2019-02-18| BOA VIAGEM|       AV BOA VIAGEM|NÃO INFORMADO|       SEM VÍTIMA|             COLISÃO|      onibus|       1|\n",
            "|2019-02-18| BOA VIAGEM|       AV BOA VIAGEM|NÃO INFORMADO|       SEM VÍTIMA|             COLISÃO|    caminhao|       1|\n",
            "|2019-03-01|   MADALENA|         RUA BENFICA|       CIDADE|       SEM VÍTIMA|ABALROAMENTO LONG...|        auto|       1|\n",
            "|2019-03-01|   MADALENA|         RUA BENFICA|       CIDADE|       SEM VÍTIMA|ABALROAMENTO LONG...|      onibus|       1|\n",
            "|2019-03-03| BOA VIAGEM|RUA PROFESSOR AUG...|NÃO INFORMADO|       SEM VÍTIMA|ABALROAMENTO LONG...|        auto|       2|\n",
            "|2019-03-19|TAMARINEIRA|            AV NORTE|     SUBURBIO|       SEM VÍTIMA|    COLISÃO TRASEIRA|        auto|       2|\n",
            "|2019-03-21| PARNAMIRIM|AV DEZESSETE DE A...|     SUBURBIO|       SEM VÍTIMA|ABALROAMENTO LONG...|        auto|       2|\n",
            "|2019-03-21| BOA VIAGEM|RUA RIBEIRO DE BRITO|     SUBURBIO|       SEM VÍTIMA|    COLISÃO TRASEIRA|        auto|       3|\n",
            "|2019-03-29|       PINA|AV CONSELHEIRO AG...|NÃO INFORMADO|       SEM VÍTIMA|             COLISÃO|        auto|       2|\n",
            "|2019-03-31|      IBURA|   AV CAMPINA GRANDE|       CIDADE|       SEM VÍTIMA|ABALROAMENTO LONG...|        auto|       2|\n",
            "|2019-04-04|     JORDÃO|  RUA JOSE MARTORANO|NÃO INFORMADO|       SEM VÍTIMA|             COLISÃO|        auto|       2|\n",
            "|2019-04-04|     JORDÃO|  RUA JOSE MARTORANO|NÃO INFORMADO|       SEM VÍTIMA|             COLISÃO|        moto|       1|\n",
            "|2019-04-07|   JAQUEIRA|   RUA DR JOSE MARIA|       CIDADE|       SEM VÍTIMA|     COLISÃO FRONTAL|        auto|       2|\n",
            "+----------+-----------+--------------------+-------------+-----------------+--------------------+------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_acidente_veiculo = df_unpivoted.join(dim_tempo, on='data', how='left') \\\n",
        "  .join(dim_localizacao, on=localizacao_columns, how='left') \\\n",
        "  .join(dim_tipo_acidente, on=acidente_columns, how='left') \\\n",
        "  .join(dim_vehicle, on='vehicle_type', how='left') \\\n",
        "  .select('tempo_key', 'localizacao_key', 'tipo_acidente_key', 'vehicle_key', 'quantity')\n",
        "\n",
        "windowSpec = Window.orderBy('tempo_key')\n",
        "\n",
        "fact_acidente_veiculo = df_acidente_veiculo.withColumn('acidente_veiculo_key', row_number().over(windowSpec))\n",
        "\n",
        "fact_acidente_veiculo.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOPf36-EWH4f",
        "outputId": "2be57609-5732-4ab0-c2e7-20b3e0afd52b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+-----------------+-----------+--------+--------------------+\n",
            "|tempo_key|localizacao_key|tipo_acidente_key|vehicle_key|quantity|acidente_veiculo_key|\n",
            "+---------+---------------+-----------------+-----------+--------+--------------------+\n",
            "| 20190101|            661|               39|          1|       2|                   1|\n",
            "| 20190101|            917|                6|          1|       2|                   2|\n",
            "| 20190101|           4233|               34|          1|       1|                   3|\n",
            "| 20190101|           4233|               34|          4|       1|                   4|\n",
            "| 20190101|           2784|               44|          1|       1|                   5|\n",
            "| 20190101|           2784|               44|          5|       1|                   6|\n",
            "| 20190101|           3130|               39|          1|       2|                   7|\n",
            "| 20190101|           2542|               44|          1|       3|                   8|\n",
            "| 20190101|           2394|                6|          1|       2|                   9|\n",
            "| 20190101|           1275|               34|          1|       1|                  10|\n",
            "| 20190101|           1275|               34|          2|       1|                  11|\n",
            "| 20190101|            790|               39|          1|       2|                  12|\n",
            "| 20190101|           3992|               54|          1|       2|                  13|\n",
            "| 20190101|           4834|               29|          1|       1|                  14|\n",
            "| 20190101|           3359|               54|          1|       2|                  15|\n",
            "| 20190101|           1475|               14|          1|       1|                  16|\n",
            "| 20190101|           3624|               41|          3|       1|                  17|\n",
            "| 20190101|           3624|               41|          6|       1|                  18|\n",
            "| 20190102|            885|               39|          1|       1|                  19|\n",
            "| 20190102|           4833|                6|          1|       1|                  20|\n",
            "+---------+---------------+-----------------+-----------+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Salvando o Data Mart de Veículos\n",
        "Finalmente, persistimos as novas tabelas na camada Gold."
      ],
      "metadata": {
        "id": "IbML16rib9Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_tables_to_save = {\n",
        "    \"dim_veiculo\": dim_vehicle,\n",
        "    \"intermediate_veiculos_por_acidente\": df_unpivoted,\n",
        "    \"fact_acidente_veiculo\": fact_acidente_veiculo\n",
        "}\n",
        "for table_name, df_object in gold_tables_to_save.items():\n",
        "  gold_table_path = f\"delta_lake/gold/{table_name}\"\n",
        "  df_object.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(gold_table_path)\n",
        "\n",
        "  print(f\"Saving '{table_name}' table to '{gold_table_path}'...\")\n",
        "\n",
        "print(f\"SUCCESS: All Gold tables saved successfully!\")"
      ],
      "metadata": {
        "id": "zXnbTZoobJq4",
        "outputId": "2eb2e566-9c47-4769-d4f1-9c9e27f4f2a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 'dim_veiculo' table to 'delta_lake/gold/dim_veiculo'...\n",
            "Saving 'intermediate_veiculos_por_acidente' table to 'delta_lake/gold/intermediate_veiculos_por_acidente'...\n",
            "Saving 'fact_acidente_veiculo' table to 'delta_lake/gold/fact_acidente_veiculo'...\n",
            "SUCCESS: All Gold tables saved successfully!\n"
          ]
        }
      ]
    }
  ]
}